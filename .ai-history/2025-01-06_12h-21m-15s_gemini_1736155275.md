# Model choice is now possible at command level

Currently, the model choice is global, at `opts` level.

I want to be able to choose the model at the command level, so that I can use different models for different commands.

If the model at command level is not set, empty string or nil, then the global model must be used.

If the model at command level is set, then the command model must be used.

Configuration exemple for setting the model at command level:

```
{
    "rakotomandimby/code-ai.nvim",
    opts = {
        gemini_model = "gemini-exp-1206",
        chatgpt_model = "gpt-40-mini",
        -- other opts
        prompts = {
            generate_readme = {
                command = "AIGenerateReadme",
                instruction_tpl = "You are a developer who is writing a README.md file for this project",
                prompt_tpl = "${input}",
                result_tpl = "${output}",
                loading_tpl = "Working on it...",
                gemini_model = "gemini-1.5-pro-latest",
                chatgpt_model = "gpt-4o",
                require_input = true                
            },
            javascript_vanilla = {
                command = 'AIJavascriptVanilla',
                instruction_tpl = 'Act as a Vanilla Javascript developer. Format you answer with Markdown.',
                prompt_tpl = '${input}',
                result_tpl = '${output}',
                loading_tpl = 'Loading...',
                gemini_model = 'gemini-2.0-flash-exp',
                chatgpt_model = 'o1-mini-preview',
                require_input = true,
            },
            php_bare = {
            command = 'AIPhpBare',
            instruction_tpl = 'Act as a PHP developer. Format you answer with Markdown.',
            prompt_tpl = '${input}',
            result_tpl = '${output}',
            loading_tpl = 'Loading...',
            require_input = true,
            },
        }
    }
}
```

With this configuration, 
- the command `AIGenerateReadme` will use the model `gemini-1.5-pro-latest`
- the command `AIJavascriptVanilla` will use the model `gemini-2.0-flash-exp`
- the command `AIPhpBare` will use the default models because the model at command level is not set

Propose me the modifications to the code in order to implement this feature of setting the model at command level.



# This is gemini-exp-1206 answer (8k in, 0k out)

```lua
-- lua/ai/init.lua
local gemini = require('ai.gemini.query')
local chatgpt = require('ai.chatgpt.query')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')

-- ... (rest of the code remains the same)

function M.handle(name, input)
  local def = M.prompts[name]
  -- ... (rest of the code remains the same)

  local gemini_model = def.gemini_model or M.opts.gemini_model
  local chatgpt_model = def.chatgpt_model or M.opts.chatgpt_model

  local function handleResult(output, output_key)
    args[output_key] = output
    args.output = (args.gemini_output or '') .. (args.chatgpt_output or '')
    update(M.fill(def.result_tpl or '${output}', args))
  end

  local askHandleResultAndCallbackGemini = {
    handleResult = function(output) return handleResult(output, 'gemini_output') end,
    callback = function() end
  }
  local askHandleResultAndCallbackChatGPT = {
    handleResult = function(output) return handleResult(output, 'chatgpt_output') end,
    callback = function() end
  }


  if (number_of_files == 0 or not use_gemini_agent or not use_chatgpt_agent) then
    common.log("Not using agents")
    gemini.ask(
      gemini_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGemini,
      M.opts.gemini_api_key)
    chatgpt.ask(
      chatgpt_model,
      instruction,
      prompt,
      askHandleResultAndCallbackChatGPT,
      M.opts.chatgpt_api_key)
  else
    common.log("Using agents")
    gemini.askHeavy(
      gemini_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGemini,
      M.opts.gemini_agent_host)
    chatgpt.askHeavy(
      chatgpt_model,
      instruction,
      prompt,
      askHandleResultAndCallbackChatGPT,
      M.opts.chatgpt_agent_host)
  end
end

-- ... (rest of the code remains the same)
```

Key changes:

1. **Model Selection:** Inside `M.handle`, two new variables `gemini_model` and `chatgpt_model` are defined. These variables use the `or` operator to prioritize the model specified in the command definition (`def.gemini_model`, `def.chatgpt_model`). If those are not present (nil or empty string), they fall back to the globally defined models in `M.opts`.

2. **Using the Selected Models:** The `gemini.ask` and `chatgpt.ask` (and their `askHeavy` counterparts) functions are now called with the dynamically determined `gemini_model` and `chatgpt_model` variables.  This ensures the correct model is used based on the command configuration.

With these changes, your provided configuration example will now correctly use the specified models at the command level, while still falling back to the global models when no specific model is configured for a command.  No other files need modification.
