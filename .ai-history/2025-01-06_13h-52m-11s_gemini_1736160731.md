# Model choice is now possible at command level

I want to be able to choose the model at the command level, so that I can use different models for different commands.
If the model at command level is not set, empty string or nil, then the global model must be used.
If the model at command level is set, then the command model must be used.

Configuration exemple for setting the model at command level:

```
{
    "rakotomandimby/code-ai.nvim",
    opts = {
        gemini_model = "gemini-exp-1206",
        chatgpt_model = "gpt-40-mini",
        -- other opts
        prompts = {
            generate_readme = {
                command = "AIGenerateReadme",
                instruction_tpl = "You are a developer who is writing a README.md file for this project",
                prompt_tpl = "${input}",
                result_tpl = "${output}",
                loading_tpl = "Working on it...",
                gemini_model = "gemini-1.5-pro-latest",
                chatgpt_model = "gpt-4o",
                require_input = true                
            },
            javascript_vanilla = {
                command = 'AIJavascriptVanilla',
                instruction_tpl = 'Act as a Vanilla Javascript developer. Format you answer with Markdown.',
                prompt_tpl = '${input}',
                result_tpl = '${output}',
                loading_tpl = 'Loading...',
                gemini_model = 'gemini-2.0-flash-exp',
                chatgpt_model = 'o1-mini-preview',
                require_input = true,
            },
            php_bare = {
            command = 'AIPhpBare',
            instruction_tpl = 'Act as a PHP developer. Format you answer with Markdown.',
            prompt_tpl = '${input}',
            result_tpl = '${output}',
            loading_tpl = 'Loading...',
            require_input = true,
            },
        }
    }
}
```

With this configuration, 
- the command `AIGenerateReadme` will use the model `gemini-1.5-pro-latest`
- the command `AIJavascriptVanilla` will use the model `gemini-2.0-flash-exp`
- the command `AIPhpBare` will use the default models because the model at command level is not set

I implemented the feature im `./lua/ai/init.lua` , tell me if this is the right way to do it.



# This is gemini-exp-1206 answer (8k in, 0k out)

```lua
local gemini = require('ai.gemini.query')
local chatgpt = require('ai.chatgpt.query')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')

local default_prompts = {
  -- ... (unchanged)
}

local M = {}
-- ... (unchanged)

-- ... (other functions unchanged)

function M.handle(name, input)
  local def = M.prompts[name]
  -- ... (unchanged)

  -- Model Selection Logic:
  local gemini_model = def.gemini_model or M.opts.gemini_model
  local chatgpt_model = def.chatgpt_model or M.opts.chatgpt_model

  -- Log the models used for debugging
  common.log("Using Gemini model: " .. gemini_model)
  common.log("Using ChatGPT model: " .. chatgpt_model)

  -- ... (rest of the function unchanged)
end


-- ... (rest of the file unchanged)
```

Your implementation in `init.lua` is correct and straightforward. By using the `or` operator, you effectively prioritize the prompt-specific model (`def.gemini_model` or `def.chatgpt_model`) and fall back to the globally configured model if the prompt-specific one isn't provided.  This is a clean and efficient way to handle the model selection logic.


The added logging lines will help you confirm which models are being used in different scenarios, which is useful for debugging and verifying the logic.  You can view the log at `/tmp/aiconfig.log`.


Consider adding a similar logging mechanism to the `query.lua` files for both Gemini and ChatGPT to track the model being used within the API calls themselves. This will provide even more granular visibility into the model selection process.  For example, in `query.ask`:

```lua
function query.ask(model, instruction, prompt, opts, api_key)
    common.log("query.ask called with model: " .. model) -- Add this line
    promptToSave = prompt
    modelUsed = model
    -- ... rest of the function
end
```
This will help you track the model being used at every stage.

