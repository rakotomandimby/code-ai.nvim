# Model choice is now possible at command level

I want to be able to choose the model at the command level, so that I can use different models for different commands.
If the model at command level is not set, empty string or nil, then the global model must be used.
If the model at command level is set, then the command model must be used.

Configuration exemple for setting the model at command level:

```
{
    "rakotomandimby/code-ai.nvim",
    opts = {
        gemini_model = "gemini-exp-1206",
        chatgpt_model = "gpt-40-mini",
        -- other opts
        prompts = {
            generate_readme = {
                command = "AIGenerateReadme",
                instruction_tpl = "You are a developer who is writing a README.md file for this project",
                prompt_tpl = "${input}",
                result_tpl = "${output}",
                loading_tpl = "Working on it...",
                gemini_model = "gemini-1.5-pro-latest",
                chatgpt_model = "gpt-4o",
                require_input = true                
            },
            javascript_vanilla = {
                command = 'AIJavascriptVanilla',
                instruction_tpl = 'Act as a Vanilla Javascript developer. Format you answer with Markdown.',
                prompt_tpl = '${input}',
                result_tpl = '${output}',
                loading_tpl = 'Loading...',
                gemini_model = 'gemini-2.0-flash-exp',
                chatgpt_model = 'o1-mini-preview',
                require_input = true,
            },
            php_bare = {
            command = 'AIPhpBare',
            instruction_tpl = 'Act as a PHP developer. Format you answer with Markdown.',
            prompt_tpl = '${input}',
            result_tpl = '${output}',
            loading_tpl = 'Loading...',
            require_input = true,
            },
        }
    }
}
```

With this configuration, 
- the command `AIGenerateReadme` will use the model `gemini-1.5-pro-latest`
- the command `AIJavascriptVanilla` will use the model `gemini-2.0-flash-exp`
- the command `AIPhpBare` will use the default models because the model at command level is not set

I implemented the feature im `./lua/ai/init.lua` , tell me if this is the right way to do it.



# This is gpt-4o-mini answer (7k in, 0k out)

To implement the feature of choosing the model at the command level in your Neovim plugin, you need to modify the `handle` function in `lua/ai/init.lua`. Below is a suggested implementation that incorporates your requirements. 

### Updated `lua/ai/init.lua`

Hereâ€™s how you can modify the `handle` function to check for command-level models:

```lua
function M.handle(name, input)
  local def = M.prompts[name]
  local width = vim.fn.winwidth(0)
  local height = vim.fn.winheight(0)
  local args = {
    locale = M.opts.locale,
    alternate_locale = M.opts.alternate_locale,
    input = input,
    input_encoded = vim.fn.json_encode(input),
  }

  local number_of_files = #aiconfig.listScannedFilesFromConfig()
  local use_gemini_agent = M.opts.gemini_agent_host ~= ''
  local use_chatgpt_agent = M.opts.chatgpt_agent_host ~= ''

  local update = nil

  if (number_of_files == 0 or not use_gemini_agent or not use_chatgpt_agent ) then
    update = M.createPopup(M.fill(def.loading_tpl , args), width - 8, height - 4)
  else
    local scanned_files = aiconfig.listScannedFiles()
    update = M.createPopup(M.fill(def.loading_tpl .. scanned_files, args), width - 8, height - 4)
  end
  local prompt = M.fill(def.prompt_tpl, args)
  local instruction = M.fill(def.instruction_tpl, args)

  -- Determine which models to use
  local gemini_model = def.gemini_model or M.opts.gemini_model
  local chatgpt_model = def.chatgpt_model or M.opts.chatgpt_model

  -- If command-level models are set, use them
  if def.gemini_model and def.gemini_model ~= '' then
    gemini_model = def.gemini_model
  end
  if def.chatgpt_model and def.chatgpt_model ~= '' then
    chatgpt_model = def.chatgpt_model
  end

  local function handleResult(output, output_key)
    args[output_key] = output
    args.output = (args.gemini_output or '') .. (args.chatgpt_output or '')
    update(M.fill(def.result_tpl or '${output}', args))
  end

  local askHandleResultAndCallbackGemini = {
    handleResult = function(output) return handleResult(output, 'gemini_output') end,
    callback = function() end
  }
  local askHandleResultAndCallbackChatGPT = {
    handleResult = function(output) return handleResult(output, 'chatgpt_output') end,
    callback = function() end
  }

  if (number_of_files == 0 or not use_gemini_agent or not use_chatgpt_agent) then
    common.log("Not using agents")
    gemini.ask(
      gemini_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGemini,
      M.opts.gemini_api_key)
    chatgpt.ask(
      chatgpt_model,
      instruction,
      prompt,
      askHandleResultAndCallbackChatGPT,
      M.opts.chatgpt_api_key)
  else
    common.log("Using agents")
    gemini.askHeavy(
      gemini_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGemini,
      M.opts.gemini_agent_host)
    chatgpt.askHeavy(
      chatgpt_model,
      instruction,
      prompt,
      askHandleResultAndCallbackChatGPT,
      M.opts.chatgpt_agent_host)
  end
end
```

### Explanation of Changes

1. **Model Selection Logic**: 
   - The code checks if `def.gemini_model` and `def.chatgpt_model` are set and not empty. If they are set, those models are used for the respective commands.
   - If they are not set, the global models (`M.opts.gemini_model` and `M.opts.chatgpt_model`) are used.

2. **Fallback Mechanism**: 
   - This implementation ensures that if a command-level model is not specified, the global model is used as a fallback.

### Conclusion

Your approach to implementing command-level model selection is correct. The modifications above ensure that the models are chosen based on the command context while maintaining the global defaults when necessary. This allows for flexibility in using different models for different commands, as you intended. 

Feel free to test this implementation and adjust any specific details as needed!

