# Feature addition: Upload LLM answers to a location.

I want to upload the answers generated by the LLM to a specific location, such as a blog or a file storage service. 
The uploaded content will be the raw response from the LLM.

Tehcnical details:

- Method: `PUT`
- Mandatory header and value: "Content-Type: text/markdown"   
- Mandatory header "X-MardkdownBlog-Token", and take the value from the configuration parameter named `upload_token`   
- Upload URL: take it from the configuration parameter named `upload_url`
- Body: the raw response from the LLM

Tell me the modifications I need to make to the code to implement this feature.




# This is gemini-2.5-pro-preview-05-06 answer (14k in, 12k out)

Okay, I will help you implement the feature to upload LLM answers. This will involve modifications to several files to handle the new configuration, process the LLM responses appropriately, and perform the upload.

Here's a summary of the changes:
1.  **`lua/ai/init.lua`**:
    *   Add new configuration options `upload_url` and `upload_token`.
    *   Implement `M.uploadContent` function to handle the `PUT` request for uploading.
    *   Modify `M.handle` to orchestrate the collection of raw LLM text and trigger the upload after all providers have responded. This includes managing active providers and ensuring upload happens only once.
2.  **`lua/ai/common.lua`**:
    *   Adjust `common.askCallback` to handle a new return structure from provider-specific `formatResult` functions. This new structure will separate the displayable text from the raw text intended for upload.
3.  **Provider-specific query files (`openai/query.lua`, `googleai/query.lua`, `anthropic/query.lua`)**:
    *   Modify their `formatResult` functions to return a table `{ display = "...", raw = "..." }`, where `raw` is the core text from the LLM and `display` is the formatted string for the Neovim popup.

Let's go through each file.

### `lua/ai/init.lua`

This file will see the most changes, including new configuration options, the upload function, and updated logic in `M.handle`.

```lua
local anthropic = require('ai.anthropic.query')
local googleai = require('ai.googleai.query')
local openai = require('ai.openai.query')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local curl = require('plenary.curl') -- Added for M.uploadContent

local default_prompts = {
  introduce = {
    command = 'AIIntroduceYourself',
    loading_tpl = 'Loading...',
    prompt_tpl = 'Say who you are, your version, and the currently used model',
    result_tpl = '${output}',
    require_input = false,
  }
}

local M = {}
M.opts = {
  anthropic_model = '',
  googleai_model = '',
  openai_model = '',

  anthropic_agent_host = '',
  googleai_agent_host = '',
  openai_agent_host = '',

  anthropic_api_key = '',
  googleai_api_key = '',
  openai_api_key = '',

  locale = 'en',
  alternate_locale = 'fr',
  result_popup_gets_focus = false,
  upload_url = '', -- New: URL for uploading LLM answers
  upload_token = '', -- New: Token for upload authentication
}
M.prompts = default_prompts
local win_id

local function splitLines(input)
  local lines = {}
  local offset = 1
  while offset > 0 do
    local i = string.find(input, '\n', offset)
    if i == nil then
      table.insert(lines, string.sub(input, offset, -1))
      offset = 0
    else
      table.insert(lines, string.sub(input, offset, i - 1))
      offset = i + 1
    end
  end
  return lines
end

local function joinLines(lines)
  local result = ""
  for _, line in ipairs(lines) do
    result = result .. line .. "\n"
  end
  return result
end

local function isEmpty(text)
  return text == nil or text == ''
end

function M.hasLetters(text)
  return type(text) == 'string' and text:match('[a-zA-Z]') ~= nil
end

function M.getSelectedText(esc)
  if esc then
    vim.api.nvim_feedkeys(vim.api.nvim_replace_termcodes('<esc>', true, false, true), 'n', false)
  end
  local vstart = vim.fn.getpos("'<")
  local vend = vim.fn.getpos("'>")
  local ok, lines = pcall(vim.api.nvim_buf_get_text, 0, vstart[2] - 1, vstart[3] - 1, vend[2] - 1, vend[3], {})
  if ok then
    return joinLines(lines)
  else
    lines = vim.api.nvim_buf_get_lines(0, vstart[2] - 1, vend[2], false)
    return joinLines(lines)
  end
end

function M.close()
  if win_id == nil or win_id == vim.api.nvim_get_current_win() then
    return
  end
  pcall(vim.api.nvim_win_close, win_id, true)
  win_id = nil
end

function M.createPopup(initialContent, width, height)
  M.close()
  local bufnr = vim.api.nvim_create_buf(false, true)

  local update = function(content)
    if content == nil then
      content = ''
    end
    local lines = splitLines(content)
    vim.bo[bufnr].modifiable = true
    vim.api.nvim_buf_set_lines(bufnr, 0, -1, true, lines)
    vim.bo[bufnr].modifiable = false
  end

  win_id = vim.api.nvim_open_win(bufnr, false, {
    relative = 'cursor',
    border = 'single',
    title = 'code-ai.md',
    style = 'minimal',
    width = width,
    height = height,
    row = 1,
    col = 0,
  })
  vim.api.nvim_buf_set_option(bufnr, 'filetype', 'markdown')
  vim.api.nvim_buf_set_option(bufnr, 'wrap', true)
  update(initialContent)
  if M.opts.result_popup_gets_focus then
    vim.api.nvim_set_current_win(win_id)
  end
  return update
end

function M.fill(tpl, args)
  if tpl == nil then
    tpl = ''
  else
    for key, value in pairs(args) do
      tpl = string.gsub(tpl, '%${' .. key .. '}', value)
    end
  end
  return tpl
end

-- New: Function to upload content
function M.uploadContent(content, url, token)
  common.log("Attempting to upload content to: " .. url)
  if not content or content == "" then
    common.log("Content is empty, skipping upload.")
    return
  end

  curl.put(url, {
    headers = {
      ['Content-Type'] = 'text/markdown',
      ['X-MardkdownBlog-Token'] = token,
    },
    body = content,
    callback = vim.schedule_wrap(function(response)
      if response.status >= 200 and response.status < 300 then
        common.log("Content uploaded successfully. Status: " .. response.status)
        vim.notify("AI content uploaded successfully.", vim.log.levels.INFO)
      else
        common.log("Failed to upload content. Status: " .. response.status .. " Body: " .. response.body)
        vim.notify("AI content upload failed. Status: " .. response.status .. ". Check logs for details.", vim.log.levels.ERROR)
      end
    end),
  })
end


function M.handle(name, input)
  local def = M.prompts[name]
  local width = vim.fn.winwidth(0)
  local height = vim.fn.winheight(0)
  local args = {
    locale = M.opts.locale,
    alternate_locale = M.opts.alternate_locale,
    input = input,
    input_encoded = vim.fn.json_encode(input),
    -- Initialize display and raw output fields
    anthropic_output = nil,
    googleai_output = nil,
    openai_output = nil,
    raw_anthropic_output = nil,
    raw_googleai_output = nil,
    raw_openai_output = nil,
  }

  local number_of_files = #aiconfig.listScannedFilesFromConfig()
  local use_anthropic_agent = M.opts.anthropic_agent_host ~= ''
  local use_googleai_agent = M.opts.googleai_agent_host ~= ''
  local use_openai_agent = M.opts.openai_agent_host ~= ''

  local update = nil

  if (number_of_files == 0 or not use_anthropic_agent or not use_googleai_agent or not use_openai_agent ) then
    update = M.createPopup(M.fill(def.loading_tpl , args), width - 8, height - 4)
  else
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    update = M.createPopup(M.fill(def.loading_tpl .. scanned_files, args), width - 8, height - 4)
  end
  local prompt = M.fill(def.prompt_tpl, args)
  local instruction = aiconfig.getSystemInstructions()

  local anthropic_model = def.anthropic_model or M.opts.anthropic_model
  local googleai_model = def.googleai_model or M.opts.googleai_model
  local openai_model = def.openai_model or M.opts.openai_model

  if def.anthropic_model and def.anthropic_model ~= '' then
    anthropic_model = def.anthropic_model
  end
  if def.googleai_model and def.googleai_model ~= '' then
    googleai_model = def.googleai_model
  end
  if def.openai_model and def.openai_model ~= '' then
    openai_model = def.openai_model
  end

  local active_providers = 0
  local completed_requests = 0

  local function check_and_upload()
    completed_requests = completed_requests + 1
    common.log("Completed requests: " .. completed_requests .. "/" .. active_providers)
    if completed_requests < active_providers then
      return -- Not all providers have responded yet
    end

    common.log("All providers responded. Proceeding with potential upload.")
    if M.opts.upload_url and M.opts.upload_url ~= '' and M.opts.upload_token and M.opts.upload_token ~= '' then
      local combined_raw_content = (args.raw_anthropic_output or "") ..
                                   (args.raw_googleai_output or "") ..
                                   (args.raw_openai_output or "")
      if combined_raw_content ~= "" then
        M.uploadContent(combined_raw_content, M.opts.upload_url, M.opts.upload_token)
      else
        common.log("Combined raw content is empty. Skipping upload.")
      end
    else
      common.log("Upload URL or token not configured. Skipping upload.")
    end
  end

  local function create_provider_handler_and_updater(provider_key_base)
    local handler = function(output_data) -- output_data is { display = "...", raw = "..." }
      common.log("Handling result for: " .. provider_key_base)
      args[provider_key_base .. '_output'] = output_data.display
      args['raw_' .. provider_key_base .. '_output'] = output_data.raw
      check_and_upload() -- This must be called to count completion
    end

    local updater = function(_) -- display_text_for_this_provider (can be ignored if args are up-to-date)
      -- Reconstruct the full display text for the popup from args
      args.output = (args.anthropic_output or '') .. (args.googleai_output or '') .. (args.openai_output or '')
      update(M.fill(def.result_tpl or '${output}', args))
    end
    return handler, updater
  end

  local anthropic_handler, anthropic_updater = create_provider_handler_and_updater('anthropic')
  local askHandleResultAndCallbackAnthropic = {
    handleResult = anthropic_handler,
    callback = anthropic_updater,
    handleError = query.formatError, -- Assuming query.formatError is available or passed if needed
  }

  local googleai_handler, googleai_updater = create_provider_handler_and_updater('googleai')
  local askHandleResultAndCallbackGoogleAI = {
    handleResult = googleai_handler,
    callback = googleai_updater,
    handleError = query.formatError,
  }

  local openai_handler, openai_updater = create_provider_handler_and_updater('openai')
  local askHandleResultAndCallbackOpenAI = {
    handleResult = openai_handler,
    callback = openai_updater,
    handleError = query.formatError,
  }

  -- Helper to call provider and manage active_providers count
  local function call_provider(provider_module, fn_name, model, instr, p_prompt, callbacks_obj, api_key_or_agent_host, provider_name_log)
    if model and model ~= "" and model ~= "disabled" then
      common.log("Requesting " .. provider_name_log .. " with model: " .. model)
      active_providers = active_providers + 1
      provider_module[fn_name](model, instr, p_prompt, callbacks_obj, api_key_or_agent_host)
    else
      common.log(provider_name_log .. " is not configured or disabled. Skipping call.")
      -- Initialize outputs to empty strings if provider is skipped, so concatenation works
      args[provider_name_log .. '_output'] = ""
      args['raw_' .. provider_name_log .. '_output'] = ""
    end
  end

  -- Determine which API to call (Light or Heavy) and make the call
  if use_anthropic_agent then
    call_provider(anthropic, "askHeavy", anthropic_model, instruction, prompt, askHandleResultAndCallbackAnthropic, M.opts.anthropic_agent_host, "anthropic")
  else
    call_provider(anthropic, "askLight", anthropic_model, instruction, prompt, askHandleResultAndCallbackAnthropic, M.opts.anthropic_api_key, "anthropic")
  end

  if use_googleai_agent then
    call_provider(googleai, "askHeavy", googleai_model, instruction, prompt, askHandleResultAndCallbackGoogleAI, M.opts.googleai_agent_host, "googleai")
  else
    call_provider(googleai, "askLight", googleai_model, instruction, prompt, askHandleResultAndCallbackGoogleAI, M.opts.googleai_api_key, "googleai")
  end

  if use_openai_agent then
    call_provider(openai, "askHeavy", openai_model, instruction, prompt, askHandleResultAndCallbackOpenAI, M.opts.openai_agent_host, "openai")
  else
    call_provider(openai, "askLight", openai_model, instruction, prompt, askHandleResultAndCallbackOpenAI, M.opts.openai_api_key, "openai")
  end
  
  -- If no providers were actually called, update the popup and exit.
  if active_providers == 0 then
    common.log("No AI providers are active for this request.")
    local no_provider_msg = "\n\nNo active AI providers configured for this request."
    if def.loading_tpl:match("%s$") then -- Add space if loading_tpl doesn't end with one
        no_provider_msg = " " .. no_provider_msg:gsub("^%s+", "")
    end
    update(M.fill(def.loading_tpl .. no_provider_msg, args))
    return 
  end
end

function M.assign(table, other)
  for k, v in pairs(other) do
    table[k] = v
  end
  return table
end

function M.setup(opts)
  for k, v in pairs(opts) do
    if k == 'prompts' then
      M.prompts = {}
      M.assign(M.prompts, default_prompts)
      M.assign(M.prompts, v)
    elseif M.opts[k] ~= nil then
      M.opts[k] = v
    else -- Log unknown options
      common.log("Unknown option in setup: " .. k)
    end
  end
  for k, v in pairs(M.prompts) do
    if v.command then
      vim.api.nvim_create_user_command(v.command, function(args_cmd) -- Renamed to avoid conflict
        local text = args_cmd['args']
        if isEmpty(text) then
          text = M.getSelectedText(true)
        end
        if not v.require_input or M.hasLetters(text) then
          M.handle(k, text)
        end
      end, { range = true, nargs = '?' })
    end
  end

  -- Check for API keys only if the corresponding model is not 'disabled' or empty
  local function check_api_key(model_opt, key_opt, provider_name)
    if M.opts[model_opt] ~= '' and M.opts[model_opt] ~= 'disabled' and M.opts[key_opt] == '' then
        -- Check if agent host is also not set for this provider
        local agent_host_key = provider_name .. "_agent_host"
        if not M.opts[agent_host_key] or M.opts[agent_host_key] == '' then
            error('You need to set ' .. key_opt .. ' if ' .. model_opt .. ' is active and no agent host is set for ' .. provider_name)
        end
    end
  end

  if (M.opts.anthropic_model == '' or M.opts.anthropic_model == 'disabled') and
     (M.opts.googleai_model == '' or M.opts.googleai_model == 'disabled') and
     (M.opts.openai_model == '' or M.opts.openai_model == 'disabled') then
    common.log('All models (anthropic_model, googleai_model, openai_model) are disabled or not set. At least one must be configured to use the plugin.')
    -- Not an error, user might intend to disable all.
  end

  check_api_key('anthropic_model', 'anthropic_api_key', 'anthropic')
  check_api_key('googleai_model', 'googleai_api_key', 'googleai')
  check_api_key('openai_model', 'openai_api_key', 'openai')


  vim.api.nvim_create_user_command('AIListScannedFiles', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    local update_popup = M.createPopup(scanned_files, width - 12, height - 8) -- Renamed to avoid conflict
    update_popup(scanned_files)
  end, {})

  vim.api.nvim_create_user_command('AIShowSystemInstructions', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local instructions = aiconfig.getSystemInstructions()
    local update_popup = M.createPopup(instructions, width - 12, height - 8) -- Renamed to avoid conflict
    update_popup(instructions)
  end, {})
end

vim.api.nvim_create_autocmd({ 'CursorMoved', 'CursorMovedI' }, {
  callback = M.close,
})

return M

```

### `lua/ai/common.lua`

This file's `askCallback` function needs to be updated to work with the new return type from provider `formatResult` functions.

```lua
local common = {}

function common.log(message)
  local log_file = io.open("/tmp/aiconfig.log", "a")
  if not log_file then
    -- Fallback to vim.notify if file logging fails, to avoid hard error
    vim.notify("Could not open log file for writing: /tmp/aiconfig.log", vim.log.levels.ERROR)
    vim.notify("Log: " .. message, vim.log.levels.WARN)
    return
  end
  -- build a timestamp string surrounded by [] that will prepend the log message
  local timestamp = os.date("%Y-%m-%d %H:%M:%S")
  message = "[ " .. timestamp .. " ] -- " .. message
  log_file:write(message .. "\n")
  log_file:close()
end


function common.askCallback(res, opts, formatResultFn)
  local result_for_popup
  local data_for_handler -- This will be the table {display="...", raw="..."}

  if res.status ~= 200 then
    local error_message
    -- Ensure opts.handleError is the function itself, not a table containing it.
    -- Based on init.lua, opts.handleError is directly passed from the provider's query module.
    if opts.handleError and type(opts.handleError) == "function" then
      error_message = opts.handleError(res.status, res.body)
    else
      common.log("Error: API responded with status " .. tostring(res.status) .. '\n\n' .. res.body)
      error_message = 'Error: API responded with status ' .. tostring(res.status) .. '\n\n' .. res.body
    end
    -- For errors, the raw and display content are the same error message.
    data_for_handler = { display = error_message, raw = error_message }
    result_for_popup = error_message
  else
    local decoded_data = vim.fn.json_decode(res.body)
    data_for_handler = formatResultFn(decoded_data) -- This now returns {display = "...", raw = "..."}
    result_for_popup = data_for_handler.display
  end

  if opts.handleResult and type(opts.handleResult) == "function" then
    -- opts.handleResult now receives the table {display = ..., raw = ...}
    -- It's responsible for any side effects (like storing raw data for upload)
    opts.handleResult(data_for_handler)
  end

  if opts.callback and type(opts.callback) == "function" then
    opts.callback(result_for_popup) -- This is for updating the popup
  end
end
return common

```

### `lua/ai/openai/query.lua`

Modify `formatResult` to return a table `{ display = "...", raw = "..." }`.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data)
  common.log("Inside OpenAI formatResult")
  local raw_text = ""
  if data and data.choices and data.choices[1] and data.choices[1].message and data.choices[1].message.content then
    raw_text = data.choices[1].message.content
  else
    raw_text = "Error: Unexpected response structure from OpenAI."
    common.log(raw_text .. " Data: " .. vim.inspect(data))
  end
  
  local usage = data.usage or { prompt_tokens = 0, completion_tokens = 0} -- Ensure usage exists
  local prompt_tokens = usage.prompt_tokens or 0
  local completion_tokens = usage.completion_tokens or 0

  local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
  local formatted_completion_tokens = string.format("%gk", math.floor(completion_tokens / 1000))

  local display_text = '\n# This is '.. modelUsed .. ' answer (' .. formatted_prompt_tokens .. ' in, ' .. formatted_completion_tokens .. ' out)\n\n'
  display_text = display_text .. raw_text .. '\n\n'
  
  if modelUsed ~= "disabled" and modelUsed ~= "" then -- Avoid saving history for disabled/empty models if not desired
      history.saveToHistory('openai_' .. modelUsed , promptToSave .. '\n\n' .. display_text)
  end
  
  return { display = display_text, raw = raw_text }
end

function query.formatError(status, body)
  common.log("Formatting OpenAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_code = error_data.error.code or ""
    local error_param = error_data.error.param or ""
    error_result = string.format("# OpenAI API Error (%s)\n\n**Error Type**: %s\n", status, error_type)
    if error_code ~= "" then
      error_result = error_result .. string.format("**Error Code**: %s\n", error_code)
    end
    if error_param ~= "" then
      error_result = error_result .. string.format("**Parameter**: %s\n", error_param)
    end
    error_result = error_result .. string.format("**Message**: %s\n", error_message)
  else
    error_result = string.format("# OpenAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  -- Pass the actual formatError function, not a table containing it.
  common.askCallback(res, {handleResult = opts.handleResult, handleError = query.formatError, callback = opts.callback}, query.formatResult)
end

local disabled_response_body = { -- Renamed to avoid conflict with other files if they were in same scope
  choices = { { message = { content = "OpenAI models are disabled" } } },
  usage = { prompt_tokens = 0, completion_tokens = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body) }, opts) end)
    return
  end
  if model == "" then -- Handle empty model string
    common.log("OpenAI model name is empty, skipping API call.")
    local empty_model_response = {
        choices = { { message = { content = "OpenAI model not specified." } } },
        usage = { prompt_tokens = 0, completion_tokens = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local url = agent_host .. '/openai'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project."})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.1})
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body) }, opts) end)
    return
  end
  if model == "" then -- Handle empty model string
    common.log("OpenAI model name is empty, skipping API call.")
    local empty_model_response = {
        choices = { { message = { content = "OpenAI model not specified." } } },
        usage = { prompt_tokens = 0, completion_tokens = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local api_host = 'https://api.openai.com'
  local path = '/v1/chat/completions'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['Authorization'] = 'Bearer ' .. api_key
      },
      body = vim.fn.json_encode(
        {
          model = model,
          messages = (function()
            local messages = {}
            if string.sub(model, 1, 2) == 'o1' then -- This seems like a specific model check, ensure it's still relevant
              table.insert(messages, {role = 'user', content = instruction .. '\n' .. prompt})
            else
              table.insert(messages, { role = 'system', content = instruction })
              table.insert(messages, {role = 'user', content = prompt})
            end
            return messages
          end)()
        }
      ),
      callback = function(res)
        common.log("Before OpenAI callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/googleai/query.lua`

Modify `formatResult` similarly.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data)
  common.log("Inside GoogleAI formatResult")
  local raw_text = ''
  local display_text = ''
  
  local usage_metadata = data.usageMetadata or {promptTokenCount = 0, candidatesTokenCount = 0} -- Ensure usageMetadata exists
  local prompt_tokens = usage_metadata.promptTokenCount or 0
  local answer_tokens = usage_metadata.candidatesTokenCount or 0

  local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
  local formatted_answer_tokens = string.format("%gk", math.floor(answer_tokens / 1000))

  if data.candidates and #data.candidates > 0 then
    if #data.candidates == 1 then
      if data.candidates[1].content and data.candidates[1].content.parts and data.candidates[1].content.parts[1] and data.candidates[1].content.parts[1].text then
        raw_text = data.candidates[1].content.parts[1].text
        display_text = '\n# This is ' .. modelUsed .. ' answer (' .. formatted_prompt_tokens .. ' in, ' .. formatted_answer_tokens .. ' out)\n\n'
        display_text = display_text .. raw_text .. '\n'
      elseif data.candidates[1].finishReason then
        raw_text = 'GoogleAI stopped with reason: ' .. data.candidates[1].finishReason
        display_text = '\n#GoogleAI error\n\n' .. raw_text .. '\n'
      else
        raw_text = "Error: Unexpected response structure from GoogleAI (single candidate)."
        display_text = '\n#GoogleAI error\n\n' .. raw_text .. '\n'
        common.log(raw_text .. " Data: " .. vim.inspect(data))
      end
    else -- Multiple candidates
      display_text = '# There are ' .. #data.candidates .. ' GoogleAI candidates\n'
      local raw_texts_combined = {}
      for i = 1, #data.candidates do
        local candidate_text = ""
        if data.candidates[i].content and data.candidates[i].content.parts and data.candidates[i].content.parts[1] and data.candidates[i].content.parts[1].text then
            candidate_text = data.candidates[i].content.parts[1].text
        else
            candidate_text = "Error: Malformed candidate " .. i
        end
        table.insert(raw_texts_combined, candidate_text)
        display_text = display_text .. '## GoogleAI Candidate number ' .. i .. '\n'
        display_text = display_text .. candidate_text .. '\n'
      end
      raw_text = table.concat(raw_texts_combined, "\n\n---\n\n") -- Combine raw texts for multiple candidates
    end
  else
    raw_text = "Error: No candidates found in GoogleAI response."
    display_text = '\n#GoogleAI error\n\n' .. raw_text .. '\n'
    if data.error then -- Check for top-level error object
        raw_text = raw_text .. " API Error: " .. (data.error.message or vim.inspect(data.error))
        display_text = display_text .. " API Error: " .. (data.error.message or vim.inspect(data.error)) .. "\n"
    end
    common.log(raw_text .. " Data: " .. vim.inspect(data))
  end
  
  if modelUsed ~= "disabled" and modelUsed ~= "" then
    history.saveToHistory('googleai_' .. modelUsed, promptToSave .. '\n\n' .. display_text)
  end
  
  return { display = display_text, raw = raw_text }
end

function query.formatError(status, body)
  common.log("Formatting GoogleAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_code = error_data.error.code or status
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_status_val = error_data.error.status or "ERROR" -- Renamed to avoid conflict
    error_result = string.format(
      "# GoogleAI API Error (%s)\n\n**Error Code**: %s\n**Status**: %s\n**Message**: %s\n",
      status,
      error_code,
      error_status_val,
      error_message
    )
  else
    error_result = string.format("# GoogleAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  common.askCallback(res, {handleResult = opts.handleResult, handleError = query.formatError, callback = opts.callback}, query.formatResult)
end

local disabled_response_body_google = { -- Renamed
  candidates = { { content = { parts = { { text = "GoogleAI models are disabled" } } }, finishReason = "STOP" } },
  usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body_google) }, opts) end)
    return
  end
  if model == "" then
    common.log("GoogleAI model name is empty, skipping API call.")
    local empty_model_response = {
        candidates = { { content = { parts = { { text = "GoogleAI model not specified." } } }, finishReason = "STOP" } },
        usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local url = agent_host .. '/googleai'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project. "})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.5})
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body_google) }, opts) end)
    return
  end
   if model == "" then
    common.log("GoogleAI model name is empty, skipping API call.")
    local empty_model_response = {
        candidates = { { content = { parts = { { text = "GoogleAI model not specified." } } }, finishReason = "STOP" } },
        usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local api_host = 'https://generativelanguage.googleapis.com'
  local path = '/v1beta/models/' .. model .. ':generateContent'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-goog-api-key'] = api_key
      },
      body = vim.fn.json_encode(
        {
          system_instruction = {parts = {text = instruction}},
          contents = {{role = 'user', parts = {{text = prompt}}}}, -- Simplified contents structure
          safetySettings = {
            { category = 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HATE_SPEECH',       threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HARASSMENT',        threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold = 'BLOCK_NONE' }
          },
          generationConfig = {
            temperature = 0.2,
            topP = 0.5
          }
        }),
      callback = function(res)
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/anthropic/query.lua`

Modify `formatResult` similarly.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data)
  common.log("Inside Anthropic formatResult")
  local raw_text = ""
  if data and data.content and data.content[1] and data.content[1].text then
    raw_text = data.content[1].text
  else
    raw_text = "Error: Unexpected response structure from Anthropic."
    common.log(raw_text .. " Data: " .. vim.inspect(data))
  end

  local usage = data.usage or { input_tokens = 0, output_tokens = 0 } -- Ensure usage exists
  local input_tokens = usage.input_tokens or 0
  local output_tokens = usage.output_tokens or 0

  local formatted_input_tokens = string.format("%gk", math.floor(input_tokens / 1000))
  local formatted_output_tokens = string.format("%gk", math.floor(output_tokens / 1000))

  local display_text = '\n# This is '.. modelUsed .. ' answer (' .. formatted_input_tokens .. ' in, ' .. formatted_output_tokens .. ' out)\n\n'
  display_text = display_text .. raw_text .. '\n\n'
  
  if modelUsed ~= "disabled" and modelUsed ~= "" then
    history.saveToHistory('claude_' .. modelUsed , promptToSave .. '\n\n' .. display_text)
  end
  
  return { display = display_text, raw = raw_text }
end

function query.formatError(status, body)
  common.log("Formatting Anthropic API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    error_result = string.format(
      "# Anthropic API Error (%s)\n\n**Error Type**: %s\n**Message**: %s\n",
      status,
      error_type,
      error_message
    )
  else
    error_result = string.format("# Anthropic API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  common.askCallback(res, {handleResult = opts.handleResult, handleError = query.formatError, callback = opts.callback}, query.formatResult)
end

local disabled_response_body_anthropic = { -- Renamed
  content = { { text = "Anthropic models are disabled" } },
  usage = { input_tokens = 0, output_tokens = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body_anthropic) }, opts) end)
    return
  end
  if model == "" then
    common.log("Anthropic model name is empty, skipping API call.")
    local empty_model_response = {
        content = { { text = "Anthropic model not specified." } },
        usage = { input_tokens = 0, output_tokens = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local url = agent_host .. '/anthropic'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project."})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.1}) -- Note: Anthropic uses top_k, not top_p for this style of API. This might be ignored or cause issues with some agent setups.
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end


function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response_body_anthropic) }, opts) end)
    return
  end
  if model == "" then
    common.log("Anthropic model name is empty, skipping API call.")
    local empty_model_response = {
        content = { { text = "Anthropic model not specified." } },
        usage = { input_tokens = 0, output_tokens = 0 }
    }
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(empty_model_response) }, opts) end)
    return
  end

  local api_host = 'https://api.anthropic.com'
  local path = '/v1/messages'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-api-key'] = api_key,
        ['anthropic-version'] = '2023-06-01'
      },
      body = vim.fn.json_encode(
        {
          model = model,
          max_tokens = 8192, -- Consider making this configurable
          system = instruction,
          messages = {{role = 'user', content = prompt}} -- Simplified messages structure
        }
      ),
      callback = function(res)
        common.log("Before Anthropic callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```
These changes should implement the desired upload feature. Remember to configure `upload_url` and `upload_token` in your Neovim setup for this plugin. I've also added checks for empty model strings in the provider files to prevent calls with invalid parameters and ensure consistent behavior. The error handling in `init.lua` for API key setup has been refined too.
