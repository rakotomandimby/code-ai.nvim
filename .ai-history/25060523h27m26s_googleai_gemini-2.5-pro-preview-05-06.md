# Feature addition: Upload LLM answers to a location.

I want to upload the answers generated by the LLM to a specific location, such as a blog or a file storage service. 
The uploaded content will be the raw response from the LLM.

Tehcnical details:

- Method: `PUT`
- Mandatory header and value: "Content-Type: text/markdown"   
- Mandatory header "X-MardkdownBlog-Token", and take the value from the configuration parameter named `upload_token`   
- Upload URL: take it from the configuration parameter named `upload_url`
- Body: the raw response from the LLM

I do not want to LLMs responses to be modified merged, I want them to be uploaded separately, one upload per response. 

Tell me the modifications I need to make to the code to implement this feature.




# This is gemini-2.5-pro-preview-05-06 answer (14k in, 10k out)

Okay, I will help you implement the feature to upload LLM answers to a specified location. This will involve modifications in several files to handle the new configuration, the upload mechanism, and ensuring raw responses are correctly passed and uploaded.

Here are the files that will be modified:

1.  `lua/ai/init.lua`: To add configuration options for upload URL and token, and to trigger the upload for each LLM response.
2.  `lua/ai/common.lua`: To add a generic upload function and to modify `askCallback` to handle raw text output alongside formatted output.
3.  `lua/ai/anthropic/query.lua`: To modify `formatResult` to return raw text output.
4.  `lua/ai/googleai/query.lua`: To modify `formatResult` to return raw text output.
5.  `lua/ai/openai/query.lua`: To modify `formatResult` to return raw text output.

Let's go through each file.

### `lua/ai/init.lua`

This file will be updated to include new options for `upload_url` and `upload_token`. The `M.handle` function will be modified to call the upload function for each LLM's response if the new options are configured.

```lua
local anthropic = require('ai.anthropic.query')
local googleai = require('ai.googleai.query')
local openai = require('ai.openai.query')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')

local default_prompts = {
  introduce = {
    command = 'AIIntroduceYourself',
    loading_tpl = 'Loading...',
    prompt_tpl = 'Say who you are, your version, and the currently used model',
    result_tpl = '${output}',
    require_input = false,
  }
}

local M = {}
M.opts = {
  anthropic_model = '',
  googleai_model = '',
  openai_model = '',

  anthropic_agent_host = '',
  googleai_agent_host = '',
  openai_agent_host = '',

  anthropic_api_key = '',
  googleai_api_key = '',
  openai_api_key = '',

  locale = 'en',
  alternate_locale = 'fr',
  result_popup_gets_focus = false,
  upload_url = '', -- New: URL for uploading LLM responses
  upload_token = '', -- New: Token for upload authentication
}
M.prompts = default_prompts
local win_id

local function splitLines(input)
  local lines = {}
  local offset = 1
  while offset > 0 do
    local i = string.find(input, '\n', offset)
    if i == nil then
      table.insert(lines, string.sub(input, offset, -1))
      offset = 0
    else
      table.insert(lines, string.sub(input, offset, i - 1))
      offset = i + 1
    end
  end
  return lines
end

local function joinLines(lines)
  local result = ""
  for _, line in ipairs(lines) do
    result = result .. line .. "\n"
  end
  return result
end

local function isEmpty(text)
  return text == nil or text == ''
end

function M.hasLetters(text)
  return type(text) == 'string' and text:match('[a-zA-Z]') ~= nil
end

function M.getSelectedText(esc)
  if esc then
    vim.api.nvim_feedkeys(vim.api.nvim_replace_termcodes('<esc>', true, false, true), 'n', false)
  end
  local vstart = vim.fn.getpos("'<")
  local vend = vim.fn.getpos("'>")
  local ok, lines = pcall(vim.api.nvim_buf_get_text, 0, vstart[2] - 1, vstart[3] - 1, vend[2] - 1, vend[3], {})
  if ok then
    return joinLines(lines)
  else
    lines = vim.api.nvim_buf_get_lines(0, vstart[2] - 1, vend[2], false)
    return joinLines(lines)
  end
end

function M.close()
  if win_id == nil or win_id == vim.api.nvim_get_current_win() then
    return
  end
  pcall(vim.api.nvim_win_close, win_id, true)
  win_id = nil
end

function M.createPopup(initialContent, width, height)
  M.close()
  local bufnr = vim.api.nvim_create_buf(false, true)

  local update = function(content)
    if content == nil then
      content = ''
    end
    local lines = splitLines(content)
    vim.bo[bufnr].modifiable = true
    vim.api.nvim_buf_set_lines(bufnr, 0, -1, true, lines)
    vim.bo[bufnr].modifiable = false
  end

  win_id = vim.api.nvim_open_win(bufnr, false, {
    relative = 'cursor',
    border = 'single',
    title = 'code-ai.md',
    style = 'minimal',
    width = width,
    height = height,
    row = 1,
    col = 0,
  })
  vim.api.nvim_buf_set_option(bufnr, 'filetype', 'markdown')
  vim.api.nvim_buf_set_option(bufnr, 'wrap', true)
  update(initialContent)
  if M.opts.result_popup_gets_focus then
    vim.api.nvim_set_current_win(win_id)
  end
  return update
end

function M.fill(tpl, args)
  if tpl == nil then
    tpl = ''
  else
    for key, value in pairs(args) do
      tpl = string.gsub(tpl, '%${' .. key .. '}', value)
    end
  end
  return tpl
end


function M.handle(name, input)
  local def = M.prompts[name]
  local width = vim.fn.winwidth(0)
  local height = vim.fn.winheight(0)
  local args = {
    locale = M.opts.locale,
    alternate_locale = M.opts.alternate_locale,
    input = input,
    input_encoded = vim.fn.json_encode(input),
  }

  local number_of_files = #aiconfig.listScannedFilesFromConfig()
  local use_anthropic_agent = M.opts.anthropic_agent_host ~= ''
  local use_googleai_agent = M.opts.googleai_agent_host ~= ''
  local use_openai_agent = M.opts.openai_agent_host ~= ''

  local update = nil

  if (number_of_files == 0 or not use_anthropic_agent or not use_googleai_agent or not use_openai_agent ) then
    update = M.createPopup(M.fill(def.loading_tpl , args), width - 8, height - 4)
  else
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    update = M.createPopup(M.fill(def.loading_tpl .. scanned_files, args), width - 8, height - 4)
  end
  local prompt = M.fill(def.prompt_tpl, args)
  local instruction = aiconfig.getSystemInstructions()

  local anthropic_model = def.anthropic_model or M.opts.anthropic_model
  local googleai_model = def.googleai_model or M.opts.googleai_model
  local openai_model = def.openai_model or M.opts.openai_model

  if def.anthropic_model and def.anthropic_model ~= '' then
    anthropic_model = def.anthropic_model
  end
  if def.googleai_model and def.googleai_model ~= '' then
    googleai_model = def.googleai_model
  end
  if def.openai_model and def.openai_model ~= '' then
    openai_model = def.openai_model
  end

  -- Modified handleResult to accept raw_text_output and model_name_for_provider
  -- It will handle UI updates and trigger uploads.
  local function handleResult(formatted_output, raw_text_output, output_key, model_name_for_provider)
    args[output_key] = formatted_output -- Store formatted output for display

    -- Upload raw_text_output if configured and model is not disabled
    if M.opts.upload_url and M.opts.upload_url ~= '' and M.opts.upload_token and M.opts.upload_token ~= '' then
      if model_name_for_provider == "disabled" then
        common.log("Skipping upload: model '" .. model_name_for_provider .. "' for provider linked to " .. output_key .. " is disabled.")
      elseif raw_text_output and raw_text_output ~= '' then
        local provider = string.gsub(output_key, "_output", "") -- "anthropic", "googleai", "openai"
        common.log("Attempting to upload content from " .. provider .. " model " .. model_name_for_provider)
        -- Call the new common.uploadContent function
        common.uploadContent(M.opts.upload_url, M.opts.upload_token, raw_text_output, provider, model_name_for_provider)
      else
        common.log("Skipping upload: raw_text_output is empty for " .. output_key)
      end
    end

    -- Aggregate outputs for display
    args.output = (args.anthropic_output or '') .. (args.googleai_output or '') .. (args.openai_output or '')
    update(M.fill(def.result_tpl or '${output}', args))
  end

  -- Modified to pass model name to the inner handleResult function
  local askHandleResultAndCallbackAnthropic = {
    handleResult = function(formatted_output, raw_text_output) -- Expects two arguments now
      return handleResult(formatted_output, raw_text_output, 'anthropic_output', anthropic_model)
    end,
    callback = function() end -- This callback is called by common.askCallback. Currently empty.
  }

  local askHandleResultAndCallbackGoogleAI = {
    handleResult = function(formatted_output, raw_text_output) -- Expects two arguments now
      return handleResult(formatted_output, raw_text_output, 'googleai_output', googleai_model)
    end,
    callback = function() end
  }

  local askHandleResultAndCallbackOpenAI = {
    handleResult = function(formatted_output, raw_text_output) -- Expects two arguments now
      return handleResult(formatted_output, raw_text_output, 'openai_output', openai_model)
    end,
    callback = function() end
  }

  if (number_of_files == 0
        or not use_anthropic_agent
        or not use_googleai_agent
        or not use_openai_agent) then
    common.log("Not using agents")
    anthropic.askLight(
      anthropic_model,
      instruction,
      prompt,
      askHandleResultAndCallbackAnthropic,
      M.opts.anthropic_api_key)
    googleai.askLight(
      googleai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGoogleAI,
      M.opts.googleai_api_key)
    openai.askLight(
      openai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackOpenAI,
      M.opts.openai_api_key)
  else
    common.log("Using agents")
    anthropic.askHeavy(
      anthropic_model,
      instruction,
      prompt,
      askHandleResultAndCallbackAnthropic,
      M.opts.anthropic_agent_host)
    googleai.askHeavy(
      googleai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGoogleAI,
      M.opts.googleai_agent_host)
    openai.askHeavy(
      openai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackOpenAI,
      M.opts.openai_agent_host)
  end
end

function M.assign(table, other)
  for k, v in pairs(other) do
    table[k] = v
  end
  return table
end

function M.setup(opts)
  for k, v in pairs(opts) do
    if k == 'prompts' then
      M.prompts = {}
      M.assign(M.prompts, default_prompts)
      M.assign(M.prompts, v)
    elseif M.opts[k] ~= nil then
      M.opts[k] = v -- This will correctly assign new options like upload_url and upload_token
    end
  end
  for k, v in pairs(M.prompts) do
    if v.command then
      vim.api.nvim_create_user_command(v.command, function(args)
        local text = args['args']
        if isEmpty(text) then
          text = M.getSelectedText(true)
        end
        if not v.require_input or M.hasLetters(text) then
          M.handle(k, text)
        end
      end, { range = true, nargs = '?' })
    end
  end

  if M.opts.anthropic_model == '' or  M.opts.googleai_model == '' or M.opts.openai_model == '' then
    error('You need to set both anthropic_model and googleai_model and openai_model')
  end
  if M.opts.anthropic_api_key ==''or M.opts.googleai_api_key == '' or M.opts.openai_api_key == '' then
    error('You need to set both anthropic_api_key and googleai_api_key and openai_api_key')
  end
  -- No error check for upload_url and upload_token, they are optional.

  vim.api.nvim_create_user_command('AIListScannedFiles', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    local update = M.createPopup(scanned_files, width - 12, height - 8)
    update(scanned_files)
  end, {})

  vim.api.nvim_create_user_command('AIShowSystemInstructions', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local instructions = aiconfig.getSystemInstructions()
    local update = M.createPopup(instructions, width - 12, height - 8)
    update(instructions)
  end, {})
end

vim.api.nvim_create_autocmd({ 'CursorMoved', 'CursorMovedI' }, {
  callback = M.close,
})

return M

```

### `lua/ai/common.lua`

This file will get a new `uploadContent` function and `askCallback` will be updated to pass both formatted and raw text to the handler.

```lua
local common = {}
local curl = require('plenary.curl') -- Ensure curl is required for the upload function

function common.log(message)
  local log_file = io.open("/tmp/aiconfig.log", "a")
  if not log_file then
    error("Could not open log file for writing.")
  end
  local timestamp = os.date("%Y-%m-%d %H:%M:%S")
  message = "[ " .. timestamp .. " ] -- " .. message
  log_file:write(message .. "\n")
  log_file:close()
end

-- New function to upload content
function common.uploadContent(url, token, content, provider_name, model_name)
  common.log("Uploading content to: " .. url .. " for provider: " .. provider_name .. ", model: " .. model_name)
  curl.put(url, {
    headers = {
      ['Content-Type'] = 'text/markdown',
      ['X-MardkdownBlog-Token'] = token,
    },
    body = content,
    callback = function(response)
      if response.status == 200 or response.status == 201 or response.status == 204 then -- Common success statuses for PUT
        common.log("Upload successful for " .. provider_name .. " model " .. model_name .. ". Status: " .. response.status)
      else
        common.log("Upload failed for " .. provider_name .. " model " .. model_name .. ". Status: " .. response.status .. " Body: " .. response.body)
      end
    end,
  })
end

-- Modified askCallback to handle two return values from formatResult
-- and pass them to opts.handleResult
function common.askCallback(res, opts, formatResult)
  local formatted_display_output
  local raw_llm_text_output -- Will hold the raw text from the LLM

  if res.status ~= 200 then
    if opts.handleError ~= nil then
      formatted_display_output = opts.handleError(res.status, res.body)
    else
      common.log("Error: API responded with the status " .. tostring(res.status) .. '\n\n' .. res.body)
      formatted_display_output = 'Error: API responded with the status ' .. tostring(res.status) .. '\n\n' .. res.body
    end
    -- In case of an error, raw_llm_text_output will be nil or an error message if handleError provides it.
    -- For now, we assume uploads are primarily for successful, raw LLM content.
    -- If opts.handleError were to return two values, we could capture a "raw error text" here too.
  else
    local data = vim.fn.json_decode(res.body)
    -- formatResult now returns two values: formatted_output, raw_text_output
    formatted_display_output, raw_llm_text_output = formatResult(data)

    if opts.handleResult ~= nil then
      -- Pass both formatted and raw text to the handler.
      -- The handler (in init.lua) is now responsible for UI updates and uploads.
      -- It no longer needs to return the formatted_display_output.
      opts.handleResult(formatted_display_output, raw_llm_text_output)
    end
  end

  -- opts.callback is still called with the formatted_display_output.
  -- In the current setup (init.lua), these callbacks are empty.
  if opts.callback then
    opts.callback(formatted_display_output)
  end
end
return common

```

### `lua/ai/anthropic/query.lua`

The `formatResult` function will be updated to return both the formatted string for display/history and the raw text content from the LLM.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified formatResult to return two values:
-- 1. The formatted string (for display, history)
-- 2. The raw text content (for upload)
function query.formatResult(data)
  common.log("Inside Anthropic formatResult")
  local raw_text_content = ""

  if data.content and data.content[1] and data.content[1].text then
    raw_text_content = data.content[1].text
  elseif data.type == "error" and data.error and data.error.message then -- Handle cases where API returns error structure in 200 OK
    raw_text_content = "Error from Anthropic: " .. data.error.message
     common.log("Anthropic API returned an error structure within a 200 OK response: " .. raw_text_content)
  elseif modelUsed == "disabled" then -- Handle disabled model case explicitly for raw content
    raw_text_content = "Anthropic models are disabled"
  else
    common.log("Anthropic formatResult: Could not extract raw text content from response data.")
    -- Potentially set raw_text_content to a generic error or empty if data structure is unexpected
  end

  local input_tokens = (data.usage and data.usage.input_tokens) or 0
  local output_tokens = (data.usage and data.usage.output_tokens) or 0

  local formatted_input_tokens = string.format("%gk", math.floor(input_tokens / 1000))
  local formatted_output_tokens = string.format("%gk", math.floor(output_tokens / 1000))

  local result = '\n# This is '.. modelUsed .. ' answer (' .. formatted_input_tokens .. ' in, ' .. formatted_output_tokens .. ' out)\n\n'
  -- Use raw_text_content for the main body of the formatted result as well, if it's the primary text
  result = result .. raw_text_content .. '\n\n'

  history.saveToHistory('claude_' .. modelUsed , promptToSave .. '\n\n' .. result)
  return result, raw_text_content -- Return both formatted result and raw text
end

function query.formatError(status, body)
  common.log("Formatting Anthropic API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    error_result = string.format(
      "# Anthropic API Error (%s)\n\n**Error Type**: %s\n**Message**: %s\n",
      status,
      error_type,
      error_message
    )
  else
    error_result = string.format("# Anthropic API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError
  common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback}, query.formatResult)
end

local disabled_response = {
  type = "message", -- Simulating a structure that formatResult can somewhat parse
  content = { { type = "text", text = "Anthropic models are disabled" } },
  usage = { input_tokens = 0, output_tokens = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/anthropic'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project."})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.1})
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end


function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://api.anthropic.com'
  local path = '/v1/messages'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-api-key'] = api_key,
        ['anthropic-version'] = '2023-06-01'
      },
      body = vim.fn.json_encode(
        {
          model = model,
          max_tokens = 8192, -- Consider making this configurable or increasing if needed
          system= instruction,
          messages = (function()
            local messages = {}
            table.insert(messages, {role = 'user', content = prompt})
            return messages
          end)()
        }
      ),
      callback = function(res)
        common.log("Before Anthropic callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/googleai/query.lua`

Similar to Anthropic, `formatResult` will be updated.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified formatResult to return two values:
-- 1. The formatted string (for display, history)
-- 2. The raw text content (for upload)
function query.formatResult(data)
  common.log("Inside GoogleAI formatResult")
  local result = ''
  local raw_text_content = ""

  if data.candidates and #data.candidates > 0 then
    local candidate = data.candidates[1]
    if candidate.content and candidate.content.parts and candidate.content.parts[1] and candidate.content.parts[1].text then
      raw_text_content = candidate.content.parts[1].text
      local prompt_tokens = (data.usageMetadata and data.usageMetadata.promptTokenCount) or 0
      local answer_tokens = (data.usageMetadata and data.usageMetadata.candidatesTokenCount) or 0
      local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
      local formatted_answer_tokens = string.format("%gk", math.floor(answer_tokens / 1000))

      result = '\n# This is ' .. modelUsed .. ' answer (' .. formatted_prompt_tokens .. ' in, ' .. formatted_answer_tokens .. ' out)\n\n'
      result = result .. raw_text_content .. '\n'
    elseif candidate.finishReason then
      raw_text_content = 'GoogleAI stopped with the reason: ' .. candidate.finishReason
      result = '\n#GoogleAI error\n\n' .. raw_text_content .. '\n'
      common.log("GoogleAI formatResult: Response stopped with reason: " .. candidate.finishReason)
    else
      raw_text_content = "GoogleAI formatResult: Could not extract text from candidate."
      result = '\n#GoogleAI error\n\n' .. raw_text_content .. '\n'
      common.log(raw_text_content)
    end
    if #data.candidates > 1 then
      -- Append other candidates to the formatted result if necessary, but raw_text_content remains the first.
      local additional_candidates_text = '\n# There are ' .. #data.candidates .. ' GoogleAI candidates (showing first)\n'
      -- For simplicity, the main 'result' and 'raw_text_content' are from the first candidate.
      -- If all candidates need to be in 'result', this part needs expansion.
      -- The request is for "raw response", typically meaning the primary one.
      result = result .. additional_candidates_text -- Or append a more detailed breakdown.
    end
  elseif modelUsed == "disabled" then -- Explicit handling for disabled model's raw content
      raw_text_content = "GoogleAI models are disabled" -- This comes from disabled_response structure
      result = '\n# This is ' .. modelUsed .. ' answer (0k in, 0k out)\n\n' .. raw_text_content .. '\n'
  else
    raw_text_content = "GoogleAI formatResult: No candidates found in response."
    result = '\n#GoogleAI error\n\n' .. raw_text_content .. '\n'
    common.log(raw_text_content)
  end

  history.saveToHistory('googleai_' .. modelUsed, promptToSave .. '\n\n' .. result)
  return result, raw_text_content -- Return both
end

function query.formatError(status, body)
  common.log("Formatting GoogleAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_code = error_data.error.code or status
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_status = error_data.error.status or "ERROR"
    error_result = string.format(
      "# GoogleAI API Error (%s)\n\n**Error Code**: %s\n**Status**: %s\n**Message**: %s\n",
      status,
      error_code,
      error_status,
      error_message
    )
  else
    error_result = string.format("# GoogleAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
    local handleError = query.formatError
    common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback}, query.formatResult)
end

local disabled_response = {
  candidates = { { content = { parts = { { text = "GoogleAI models are disabled" } } }, finishReason = "STOP" } },
  usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/googleai'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project. "})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.5})
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://generativelanguage.googleapis.com'
  local path = '/v1beta/models/' .. model .. ':generateContent'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-goog-api-key'] = api_key
      },
      body = vim.fn.json_encode(
        {
          system_instruction = {parts = {text = instruction}},
          contents = (function()
            local contents = {}
            table.insert(contents, {role = 'user', parts = {{text = prompt}}})
            return contents
          end)(),
          safetySettings = {
            { category = 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HATE_SPEECH',       threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HARASSMENT',        threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold = 'BLOCK_NONE' }
          },
          generationConfig = {
            temperature = 0.2,
            topP = 0.5
          }
        }),
      callback = function(res)
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/openai/query.lua`

And finally, `formatResult` in the OpenAI query module will also be updated.

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified formatResult to return two values:
-- 1. The formatted string (for display, history)
-- 2. The raw text content (for upload)
function query.formatResult(data)
  common.log("Inside OpenAI formatResult")
  local raw_text_content = ""

  if data.choices and data.choices[1] and data.choices[1].message and data.choices[1].message.content then
    raw_text_content = data.choices[1].message.content
  elseif modelUsed == "disabled" then -- Handle disabled model case explicitly for raw content
    raw_text_content = "OpenAI models are disabled" -- This comes from disabled_response structure
  else
    common.log("OpenAI formatResult: Could not extract raw text content from response data.")
    -- Potentially set raw_text_content to a generic error or empty if data structure is unexpected
  end

  local prompt_tokens = (data.usage and data.usage.prompt_tokens) or 0
  local completion_tokens = (data.usage and data.usage.completion_tokens) or 0

  local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
  local formatted_completion_tokens = string.format("%gk", math.floor(completion_tokens / 1000))

  local result = '\n# This is '.. modelUsed .. ' answer (' .. formatted_prompt_tokens .. ' in, ' .. formatted_completion_tokens .. ' out)\n\n'
  result = result .. raw_text_content .. '\n\n'

  history.saveToHistory('openai_' .. modelUsed , promptToSave .. '\n\n' .. result)
  return result, raw_text_content -- Return both
end

function query.formatError(status, body)
  common.log("Formatting OpenAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_code = error_data.error.code or ""
    local error_param = error_data.error.param or ""
    error_result = string.format("# OpenAI API Error (%s)\n\n**Error Type**: %s\n", status, error_type)
    if error_code ~= "" then
      error_result = error_result .. string.format("**Error Code**: %s\n", error_code)
    end
    if error_param ~= "" then
      error_result = error_result .. string.format("**Parameter**: %s\n", error_param)
    end
    error_result = error_result .. string.format("**Message**: %s\n", error_message)
  else
    error_result = string.format("# OpenAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError
  common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback}, query.formatResult)
end

local disabled_response = {
  choices = { { message = { role = "assistant", content = "OpenAI models are disabled" } } }, -- Ensure message.content exists
  usage = { prompt_tokens = 0, completion_tokens = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, agent_host)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/openai'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project."})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = 0.2})
  table.insert(body_chunks, {top_p = 0.1})
  table.insert(body_chunks, {})

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

function query.askLight(model, instruction, prompt, opts, api_key)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://api.openai.com'
  local path = '/v1/chat/completions'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['Authorization'] = 'Bearer ' .. api_key
      },
      body = vim.fn.json_encode(
        {
          model = model,
          messages = (function()
            local messages = {}
            if string.sub(model, 1, 2) == 'o1' then -- This seems like a custom model prefix, ensure it's handled
              table.insert(messages, {role = 'user', content = instruction .. '\n' .. prompt})
            else
              table.insert(messages, { role = 'system', content = instruction })
              table.insert(messages, {role = 'user', content = prompt})
            end
            return messages
          end)()
        }
      ),
      callback = function(res)
        common.log("Before OpenAI callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

These changes should implement the desired feature. Remember to configure `upload_url` and `upload_token` in your Neovim setup for this plugin. For example:

```lua
require('ai').setup({
  -- ... your existing options ...
  anthropic_model = "claude-3-haiku-20240307", -- or your preferred model
  googleai_model = "gemini-1.5-flash-latest", -- or your preferred model
  openai_model = "gpt-3.5-turbo", -- or your preferred model

  anthropic_api_key = "YOUR_ANTHROPIC_KEY",
  googleai_api_key = "YOUR_GOOGLEAI_KEY",
  openai_api_key = "YOUR_OPENAI_KEY",

  upload_url = "YOUR_UPLOAD_URL_HERE",
  upload_token = "YOUR_UPLOAD_TOKEN_HERE",
  -- ... other options ...
})
```
Make sure the `upload_url` endpoint is prepared to receive `PUT` requests with a `text/markdown` body and the specified headers.
