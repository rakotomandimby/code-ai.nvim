# Code review 

Tell me your analysis of the codebase.


# GGL Codebase Analysis and Review

I have performed a comprehensive code review of the provided Lua codebase for your Neovim AI plugin. The project is well-structured into logical modules, demonstrating a clear separation of concerns. The implementation covers core functionalities like configuration management, project context scanning, interaction with multiple LLM providers (Anthropic, GoogleAI, OpenAI), history logging, and a basic Neovim UI.

My analysis covers the overall quality, readability, error handling, performance considerations, Neovim integration, and potential areas for improvement across all files.

## Overall Summary

The codebase is generally well-written and functional, with good modularity and a clear intent for each file. It leverages Neovim's API and `plenary.curl` effectively for asynchronous network requests. Error handling for API responses is robust, and the history and configuration management are thoughtfully implemented.

However, there are opportunities to enhance robustness, improve user experience, reduce code duplication, and optimize certain synchronous operations for better performance in larger projects. Specific areas for improvement include the `getProjectRoot` logic, the `askHeavy` agent communication pattern, the aggressiveness of the popup closing autocmd, and making more parameters configurable.

---

## File-by-File Analysis

### `lua/ai/aiconfig.lua`

This file is responsible for finding system instructions, scanned files configuration, and determining the project root.

**Good Points:**
*   **Clear Responsibilities:** Well-defined functions for specific configuration and project-related tasks.
*   **`getProjectRoot`:** Implements a sensible hierarchy for detecting the project root, starting with plugin-specific config files and falling back to common VCS/project files.
*   **`listScannedFilesFromConfig`:** Handles include/exclude glob patterns effectively and sorts files by size, which is useful for prioritizing content.
*   **`getSystemInstructions`:** The `append_embeded` parameter adds flexibility for users to control system instruction merging.

**Potential Issues and Improvements:**
*   **`getProjectRoot` Logic:** The current logic finds the *first* match upwards. If a `.ai-scanned-files` file exists in a subdirectory, that subdirectory becomes the project root, which might not be the intended overall project root. I suggest considering a strategy where the highest-level `.git` or `README.md` defines the primary project root, and then `.ai-scanned-files` is sought within that root, or allow explicit user configuration of the project root.
*   **`listScannedFilesFromConfig` Performance:** `vim.fn.glob` is synchronous and can be slow for large projects or complex glob patterns. For very large codebases, this could block Neovim. I recommend exploring `vim.loop.fs_scandir` for asynchronous directory scanning or `plenary.path` for more optimized file system operations.
*   **`getSystemInstructions` Fallback:** While `vim.api.nvim_get_runtime_file` is the primary and correct way to find plugin files, the `debug.getinfo` fallback is a bit fragile. Ensuring the runtime path is correctly set up is key.
*   **`listScannedFilesAsFormattedTable`:** This function performs two passes over the `files_data` (one for size sort, one for name sort). It could be slightly optimized by creating two separate sorted lists directly after the initial data gathering pass.

### `lua/ai/init.lua`

This is the main entry point for the plugin, handling setup, user command registration, and the UI popup.

**Good Points:**
*   **Structured Options:** `M.opts` provides a clear structure for plugin configuration.
*   **Prompt Definitions:** `M.prompts` allows for defining custom AI commands with templates.
*   **UI Consistency:** `createPopup` centralizes the logic for displaying AI responses in a consistent Neovim floating window.
*   **`handle` Function:** Orchestrates the entire AI query process, from UI display to calling the appropriate LLM provider.
*   **`setup` Function:** Correctly registers user commands and performs initial validation of essential configuration options.

**Potential Issues and Improvements:**
*   **`getSelectedText`:** The `esc` parameter is a workaround. It's generally better to ensure visual mode is exited *before* calling this function if that's the intent, or to handle the selection more robustly without relying on `feedkeys`.
*   **`handle` Function Complexity:** This function is quite large and performs many steps. I suggest breaking it down into smaller, more focused helper functions to improve readability and maintainability. For example, the `askHandleResultAndCallbackX` tables are highly repetitive and could be generated by a factory function.
*   **Model/API Key Validation:** The `setup` function currently errors if *any* model or API key is missing. This is too strict if a user only intends to use one LLM provider and disables others. I recommend modifying the validation to only check for keys/models of *enabled* providers.
*   **Aggressive Popup Closing:** The `CursorMoved` and `CursorMovedI` autocmds for `M.close` are very aggressive. This will close the popup on almost any cursor movement, which can be disruptive. I suggest a less intrusive approach, such as closing on `BufLeave` for the popup buffer, or providing a dedicated command to close it, or making this behavior configurable.
*   **`M.assign` for Prompts:** `M.assign` performs a shallow copy. If `default_prompts` contains nested tables, modifications from user `opts.prompts` might unintentionally alter the default prompt structure. Consider using `vim.tbl_deep_extend` for merging nested tables.

### `lua/ai/googleai/query.lua`, `lua/ai/openai/query.lua`, `lua/ai/anthropic/query.lua`

These files handle the specific API interactions for each LLM provider.

**Good Points:**
*   **Clear Separation:** Each provider has its own module, promoting modularity.
*   **`askLight` vs. `askHeavy`:** Distinguishes between direct API calls and agent-based interactions, which is a good design choice.
*   **Consistent Formatting:** `formatResult` and `formatError` functions are consistently implemented across providers, providing useful token counts and detailed error messages.
*   **Disabled Model Handling:** Gracefully handles "disabled" models without making API calls.
*   **Upload Integration:** Seamlessly integrates with `common.uploadContent` for result archiving.

**Potential Issues and Improvements:**
*   **Repetitive `askCallback` Parameters:** The `upload_url`, `upload_token`, and `upload_as_public` parameters are passed through many function calls. This verbosity could be reduced by encapsulating these into a single `query_options` table or making them globally accessible via `M.opts` where appropriate.
*   **`askHeavy` Agent Communication:** The `body_chunks` and `sendNextRequest` approach in `askHeavy` implies a sequential sending of messages to the agent. This is an unusual pattern for LLM APIs, which typically expect a single, complete conversation history. It's crucial to verify that this communication protocol matches the agent's expected input. If the agent expects a single payload, this logic needs refactoring. The final empty `table.insert(body_chunks, {})` in Anthropic and GoogleAI `askHeavy` is also unclear in its purpose.
*   **OpenAI `askLight` Model Check:** The `if string.sub(model, 1, 2) == 'o1' or string.sub(model, 1, 2) == 'o3' or string.sub(model, 1, 2) == 'o4'` condition for system message placement is brittle. It relies on specific model naming conventions. A more robust solution would be to configure which models support a dedicated system role.
*   **Hardcoded Parameters:** `max_tokens` for Anthropic and `temperature`/`topP` values are hardcoded. These should be configurable, either globally or per prompt, to allow users to fine-tune model behavior.

### `lua/ai/common.lua`

This file contains general utility functions used across the plugin.

**Good Points:**
*   **Centralized Logging:** `common.log` provides a consistent way to log messages, using native Lua I/O for better performance than `vim.fn.writefile`.
*   **Generic API Callback:** `common.askCallback` effectively abstracts the common logic for handling API responses, including error formatting and result processing.
*   **Content Upload:** `uploadContent` provides a robust mechanism for uploading results to an external service, including the new `is_public` option.
*   **Title Formatting:** `insertWordToTitle` ensures consistent formatting of output titles.

**Potential Issues and Improvements:**
*   **`common.log` Path:** The log path `/tmp/aiconfig.log` is fixed. I recommend making this configurable or using `vim.fn.stdpath('cache')` to place it in a user-specific temporary directory, which is more standard.
*   **`uploadContent` Redundancy:** The `model_name ~= 'disabled'` check within `uploadContent` is redundant if the calling `query` functions already handle the disabled state before invoking `uploadContent`.

### `lua/ai/history.lua`

This module manages the saving and cleanup of AI interaction history.

**Good Points:**
*   **Project-Specific History:** Creates a `.ai-history` directory under the project root, keeping history organized per project.
*   **Timestamped Files:** Uses timestamped filenames for easy chronological sorting.
*   **History Cleanup:** `removeOldestHistoryFiles` provides a mechanism to prevent the history directory from growing indefinitely.

**Potential Issues and Improvements:**
*   **`removeOldestHistoryFiles` Performance:** For very large history directories (thousands of files), `vim.fn.readdir` and `table.sort` could become slow. While unlikely to be a major issue for typical usage, for extreme cases, a more optimized cleanup strategy (e.g., deleting files older than a certain date without listing all files) could be considered.

### `lua/ai/globpattern.lua`

This module provides a function to convert glob patterns into Lua patterns.

**Good Points:**
*   **Dedicated Functionality:** Encapsulates the glob-to-pattern conversion logic in a separate module.
*   **Basic Glob Support:** Handles common glob characters (`*`, `?`, `[]`) and backslash escaping.

**Potential Issues and Improvements:**
*   **Complexity:** The `globtopattern` function is quite complex due to manual string manipulation and index tracking. This makes it prone to subtle bugs and difficult to maintain.
*   **Robustness:** While it covers basic cases, robust glob-to-regex conversion can be tricky due to differences in various glob/regex dialects and edge cases (e.g., character classes, nested patterns). I suggest thoroughly testing this function with a wide range of patterns. If issues arise, consider using a well-tested external library if available, or simplifying the requirements for glob patterns.

---

## Proposed Changes

Based on the analysis, I propose the following changes to improve the codebase. I will provide the full content of the modified files.

**Files to be Modified:**
1.  `lua/ai/init.lua`
2.  `lua/ai/googleai/query.lua`
3.  `lua/ai/openai/query.lua`
4.  `lua/ai/anthropic/query.lua`
5.  `lua/ai/common.lua`

**Summary of Proposed Changes:**

*   **`lua/ai/init.lua`**:
    *   Refactor `handle` to reduce complexity by introducing helper functions for creating `askHandleResultAndCallback` tables.
    *   Adjust model/API key validation in `setup` to only check for *enabled* models.
    *   Modify the popup closing autocmd to be less aggressive, closing on `BufLeave` instead of `CursorMoved`.
    *   Improve `M.assign` for prompts to use `vim.tbl_deep_extend` for better merging of nested tables.
    *   Clarify the `handle` function's logic for displaying scanned files.
*   **`lua/ai/googleai/query.lua`, `lua/ai/openai/query.lua`, `lua/ai/anthropic/query.lua`**:
    *   Remove the redundant `upload_url`, `upload_token`, `upload_as_public` parameters from `askLight` and `askHeavy` by passing them as part of the `opts` table, which already contains these. This simplifies function signatures.
    *   Remove the final empty `table.insert(body_chunks, {})` in `askHeavy` as its purpose is unclear and it might send an unnecessary empty message to the agent.
    *   Make `max_tokens`, `temperature`, and `topP` configurable via `opts` for `askLight` and `askHeavy` functions.
*   **`lua/ai/common.lua`**:
    *   Make the log file path configurable via `M.opts` in `init.lua` and use `vim.fn.stdpath('cache')` as a default.
    *   Remove the redundant `model_name ~= 'disabled'` check in `uploadContent`.

---
### `lua/ai/init.lua`

```lua
local anthropic = require('ai.anthropic.query')
local googleai = require('ai.googleai.query')
local openai = require('ai.openai.query')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')

local default_prompts = {
  introduce = {
    command = 'AIIntroduceYourself',
    loading_tpl = 'Loading...',
    prompt_tpl = 'Say who you are, your version, and the currently used model',
    result_tpl = '${output}',
    require_input = false,
  }
}

local M = {}
M.opts = {
  anthropic_model = '',
  googleai_model = '',
  openai_model = '',

  anthropic_agent_host = '',
  googleai_agent_host = '',
  openai_agent_host = '',

  anthropic_api_key = '',
  googleai_api_key = '',
  openai_api_key = '',

  locale = 'en',
  alternate_locale = 'fr',
  result_popup_gets_focus = false,
  upload_url = '',
  upload_token = '',
  upload_as_public = false,
  append_embeded_system_instructions = true,
  log_file_path = vim.fn.stdpath('cache') .. '/ai_plugin.log', -- New configurable log path
  default_max_tokens = 8192, -- New configurable default max tokens
  default_temperature = 0.2, -- New configurable default temperature
  default_top_p = 0.5, -- New configurable default top_p
}
M.prompts = default_prompts
local win_id

local function splitLines(input)
  local lines = {}
  local offset = 1
  while offset > 0 do
    local i = string.find(input, '\n', offset)
    if i == nil then
      table.insert(lines, string.sub(input, offset, -1))
      offset = 0
    else
      table.insert(lines, string.sub(input, offset, i - 1))
      offset = i + 1
    end
  end
  return lines
end

local function joinLines(lines)
  local result = ""
  for _, line in ipairs(lines) do
    result = result .. line .. "\n"
  end
  return result
end

local function isEmpty(text)
  return text == nil or text == ''
end

function M.hasLetters(text)
  return type(text) == 'string' and text:match('[a-zA-Z]') ~= nil
end

function M.getSelectedText(esc)
  if esc then
    vim.api.nvim_feedkeys(vim.api.nvim_replace_termcodes('<esc>', true, false, true), 'n', false)
  end
  local vstart = vim.fn.getpos("'<")
  local vend = vim.fn.getpos("'>")
  local ok, lines = pcall(vim.api.nvim_buf_get_text, 0, vstart[2] - 1, vstart[3] - 1, vend[2] - 1, vend[3], {})
  if ok then
    return joinLines(lines)
  else
    -- Fallback for older Neovim versions or specific selection types
    lines = vim.api.nvim_buf_get_lines(0, vstart[2] - 1, vend[2], false)
    return joinLines(lines)
  end
end

function M.close()
  if win_id == nil or win_id == vim.api.nvim_get_current_win() then
    return
  end
  pcall(vim.api.nvim_win_close, win_id, true)
  win_id = nil
end

function M.createPopup(initialContent, width, height)
  M.close()
  local bufnr = vim.api.nvim_create_buf(false, true)

  local update = function(content)
    if content == nil then
      content = ''
    end
    local lines = splitLines(content)
    vim.bo[bufnr].modifiable = true
    vim.api.nvim_buf_set_lines(bufnr, 0, -1, true, lines)
    vim.bo[bufnr].modifiable = false
  end

  win_id = vim.api.nvim_open_win(bufnr, false, {
    relative = 'cursor',
    border = 'single',
    title = 'code-ai.md',
    style = 'minimal',
    width = width,
    height = height,
    row = 1,
    col = 0,
  })
  vim.bo[bufnr].filetype = 'markdown'
  vim.api.nvim_win_set_option(win_id, 'wrap', true)

  -- Autocmd to close the popup when leaving its buffer
  vim.api.nvim_create_autocmd('BufLeave', {
    buffer = bufnr,
    callback = M.close,
    once = true,
    desc = 'Close AI popup on buffer leave',
  })

  update(initialContent)
  if M.opts.result_popup_gets_focus then
    vim.api.nvim_set_current_win(win_id)
  end
  return update
end

function M.fill(tpl, args)
  if tpl == nil then
    tpl = ''
  else
    for key, value in pairs(args) do
      tpl = string.gsub(tpl, '%${' .. key .. '}', value)
    end
  end
  return tpl
end

-- Helper function to create common query options for LLM providers
local function create_llm_query_opts(handle_result_func, output_key)
  return {
    handleResult = function(output) return handle_result_func(output, output_key) end,
    callback = function() end, -- Placeholder, actual callback is handled by common.askCallback
    upload_url = M.opts.upload_url,
    upload_token = M.opts.upload_token,
    upload_as_public = M.opts.upload_as_public,
    max_tokens = M.opts.default_max_tokens,
    temperature = M.opts.default_temperature,
    top_p = M.opts.default_top_p,
  }
end

function M.handle(name, input)
  local def = M.prompts[name]
  local width = vim.fn.winwidth(0)
  local height = vim.fn.winheight(0)
  local args = {
    locale = M.opts.locale,
    alternate_locale = M.opts.alternate_locale,
    input = input,
    input_encoded = vim.fn.json_encode(input),
  }

  local number_of_files = #aiconfig.listScannedFilesFromConfig()
  local use_anthropic_agent = M.opts.anthropic_agent_host ~= ''
  local use_googleai_agent = M.opts.googleai_agent_host ~= ''
  local use_openai_agent = M.opts.openai_agent_host ~= ''

  local update = nil
  local initial_popup_content = M.fill(def.loading_tpl, args)

  -- Only append scanned files if there are files and at least one agent is configured
  -- This ensures the table is shown when agents are potentially used.
  if number_of_files > 0 and (use_anthropic_agent or use_googleai_agent or use_openai_agent) then
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    initial_popup_content = initial_popup_content .. "\n\n" .. scanned_files
  end
  update = M.createPopup(initial_popup_content, width - 8, height - 4)

  local prompt = M.fill(def.prompt_tpl, args)

  local append_embeded = M.opts.append_embeded_system_instructions
  if def.append_embeded_system_instructions ~= nil then
    append_embeded = def.append_embeded_system_instructions
  end
  local instruction = aiconfig.getSystemInstructions(append_embeded)

  local anthropic_model = def.anthropic_model or M.opts.anthropic_model
  local googleai_model = def.googleai_model or M.opts.googleai_model
  local openai_model = def.openai_model or M.opts.openai_model

  -- Override default generation config with prompt-specific values if available
  local query_config = {
    max_tokens = def.max_tokens or M.opts.default_max_tokens,
    temperature = def.temperature or M.opts.default_temperature,
    top_p = def.top_p or M.opts.default_top_p,
  }

  local function handleResult(output, output_key)
    args[output_key] = output
    args.output = (args.anthropic_output or '').. (args.googleai_output or '') .. (args.openai_output or '')
    update(M.fill(def.result_tpl or '${output}', args))
  end

  local askHandleResultAndCallbackAnthropic = create_llm_query_opts(handleResult, 'anthropic_output')
  local askHandleResultAndCallbackGoogleAI = create_llm_query_opts(handleResult, 'googleai_output')
  local askHandleResultAndCallbackOpenAI = create_llm_query_opts(handleResult, 'openai_output')

  local use_heavy_agent = number_of_files > 0 and (use_anthropic_agent or use_googleai_agent or use_openai_agent)

  if not use_heavy_agent then
    common.log("Not using agents (light query)")
    anthropic.askLight(
      anthropic_model,
      instruction,
      prompt,
      askHandleResultAndCallbackAnthropic,
      M.opts.anthropic_api_key,
      query_config -- Pass query config
    )
    googleai.askLight(
      googleai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGoogleAI,
      M.opts.googleai_api_key,
      query_config -- Pass query config
    )
    openai.askLight(
      openai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackOpenAI,
      M.opts.openai_api_key,
      query_config -- Pass query config
    )
  else
    common.log("Using agents (heavy query)")
    anthropic.askHeavy(
      anthropic_model,
      instruction,
      prompt,
      askHandleResultAndCallbackAnthropic,
      M.opts.anthropic_api_key,
      M.opts.anthropic_agent_host,
      query_config -- Pass query config
    )
    googleai.askHeavy(
      googleai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackGoogleAI,
      M.opts.googleai_api_key,
      M.opts.googleai_agent_host,
      query_config -- Pass query config
    )
    openai.askHeavy(
      openai_model,
      instruction,
      prompt,
      askHandleResultAndCallbackOpenAI,
      M.opts.openai_api_key,
      M.opts.openai_agent_host,
      query_config -- Pass query config
    )
  end
end

-- Using vim.tbl_deep_extend for better merging of nested tables
function M.assign(target, source)
  return vim.tbl_deep_extend('force', target, source)
end

function M.setup(opts)
  -- Set the log file path first, so common.log uses the correct path
  if opts.log_file_path then
    M.opts.log_file_path = opts.log_file_path
  end
  common.set_log_path(M.opts.log_file_path) -- Update common module with the log path

  for k, v in pairs(opts) do
    if k == 'prompts' then
      M.prompts = M.assign({}, default_prompts) -- Start with a fresh copy of defaults
      M.prompts = M.assign(M.prompts, v)
    elseif M.opts[k] ~= nil then
      M.opts[k] = v
    end
  end

  -- Validate API keys and models only for enabled providers
  if M.opts.anthropic_model ~= '' and M.opts.anthropic_model ~= 'disabled' then
    if M.opts.anthropic_api_key == '' then
      error('Anthropic model is enabled, but anthropic_api_key is not set.')
    end
  end
  if M.opts.googleai_model ~= '' and M.opts.googleai_model ~= 'disabled' then
    if M.opts.googleai_api_key == '' then
      error('GoogleAI model is enabled, but googleai_api_key is not set.')
    end
  end
  if M.opts.openai_model ~= '' and M.opts.openai_model ~= 'disabled' then
    if M.opts.openai_api_key == '' then
      error('OpenAI model is enabled, but openai_api_key is not set.')
    end
  end

  for k, v in pairs(M.prompts) do
    if v.command then
      vim.api.nvim_create_user_command(v.command, function(args)
        local text = args['args']
        if isEmpty(text) then
          text = M.getSelectedText(true)
        end
        if not v.require_input or M.hasLetters(text) then
          M.handle(k, text)
        end
      end, { range = true, nargs = '?' })
    end
  end

  vim.api.nvim_create_user_command('AIListScannedFiles', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local scanned_files = aiconfig.listScannedFilesAsFormattedTable()
    local update = M.createPopup(scanned_files, width - 12, height - 8)
    update(scanned_files)
  end, {})

  vim.api.nvim_create_user_command('AIShowSystemInstructions', function()
    local width = vim.fn.winwidth(0)
    local height = vim.fn.winheight(0)
    local instructions = aiconfig.getSystemInstructions()
    local update = M.createPopup(instructions, width - 12, height - 8)
    update(instructions)
  end, {})
end

-- Removed aggressive CursorMoved autocmd. Popup now closes on BufLeave.
-- vim.api.nvim_create_autocmd({ 'CursorMoved', 'CursorMovedI' }, {
--   callback = M.close,
-- })

return M

```

### `lua/ai/googleai/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified: Removed upload_url, upload_token, and upload_as_public parameters from formatResult
function query.formatResult(data, opts)
  common.log("Inside GoogleAI formatResult")
  local result = ''
  local candidates_number = #data['candidates']
  if candidates_number == 1 then
    if data['candidates'][1]['content'] == nil then
      result = '\n#GoogleAI error\n\nGoogleAI stopped with the reason: ' .. data['candidates'][1]['finishReason'] .. '\n'
      -- No upload for error
      return result
    else
      -- Extract token counts from the response
      local prompt_tokens = data['usageMetadata']['promptTokenCount'] or 0  -- Default to 0
      local answer_tokens = data['usageMetadata']['candidatesTokenCount'] or 0 -- Default to 0

      -- Format token counts (e.g., "30k", "2k")
      local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
      local formatted_answer_tokens = string.format("%gk", math.floor(answer_tokens / 1000))

      result = result .. data['candidates'][1]['content']['parts'][1]['text'] .. '\n\n' .. 'GoogleAI ' .. modelUsed .. ' (' .. formatted_prompt_tokens .. ' in, ' .. formatted_answer_tokens .. ' out)\n\n'
    end
  else
    result = '# There are ' .. candidates_number .. ' GoogleAI candidates\n'
    for i = 1, candidates_number do
      result = result .. '## GoogleAI Candidate number ' .. i .. '\n'
      result = result .. data['candidates'][i]['content']['parts'][1]['text'] .. '\n'
    end
  end
  result = common.insertWordToTitle('GGL', result)
  history.saveToHistory('googleai_' .. modelUsed  , promptToSave .. '\n\n' .. result)

  -- START: Upload the formatted result with public option, using opts
  common.uploadContent(opts.upload_url, opts.upload_token, result, 'GoogleAI (' .. modelUsed .. ')', opts.upload_as_public)
  -- END: Upload the formatted result with public option

  return result
end

-- Added a new function to handle and format GoogleAI API errors
function query.formatError(status, body)
  common.log("Formatting GoogleAI API error: " .. body)
  local error_result
  -- Try to parse the error JSON
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    -- Extract specific error information
    local error_code = error_data.error.code or status
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_status = error_data.error.status or "ERROR"
    error_result = string.format(
      "# GoogleAI API Error (%s)\n\n**Error Code**: %s\n**Status**: %s\n**Message**: %s\n",
      status,
      error_code,
      error_status,
      error_message
    )
  else
    -- Fallback for unexpected error format
    error_result = string.format("# GoogleAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
    local handleError = query.formatError  -- Set our custom error handler
    -- Modified: Pass opts directly to common.askCallback, which then passes it to formatResult
    common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback, upload_url = opts.upload_url, upload_token = opts.upload_token, upload_as_public = opts.upload_as_public}, query.formatResult)
end

local disabled_response = {
  candidates = { { content = { parts = { { text = "GoogleAI models are disabled" } } }, finishReason = "STOP" } },
  usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
}

-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, query_config)
  promptToSave = prompt
  modelUsed = model

  -- Check if model is disabled
  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/googleai'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {api_key = api_key})
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project. "})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = query_config.temperature or 0.2}) -- Use configurable temperature
  table.insert(body_chunks, {top_p = query_config.top_p or 0.5}) -- Use configurable top_p
  -- Removed the final empty table.insert(body_chunks, {}) as its purpose was unclear

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askLight(model, instruction, prompt, opts, api_key, query_config)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://generativelanguage.googleapis.com'
  -- local api_host = 'https://eowloffrpvxwtqp.m.pipedream.net'
  local path = '/v1beta/models/' .. model .. ':generateContent'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-goog-api-key'] = api_key
      },
      body = vim.fn.json_encode(
        {
          system_instruction = {parts = {text = instruction}},
          contents = (function()
            local contents = {}
            table.insert(contents, {role = 'user', parts = {{text = prompt}}})
            return contents
          end)(),
          safetySettings = {
            { category = 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HATE_SPEECH',       threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_HARASSMENT',        threshold = 'BLOCK_NONE' },
            { category = 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold = 'BLOCK_NONE' }
          },
          generationConfig = {
            temperature = query_config.temperature or 0.2, -- Use configurable temperature
            topP = query_config.top_p or 0.5 -- Use configurable top_p
          }
        }),
      callback = function(res)
        -- common.log("Before GoogleAI callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/openai/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified: Removed upload_url, upload_token, and upload_as_public parameters from formatResult
function query.formatResult(data, opts)
  common.log("Inside OpenAI formatResult")
  local prompt_tokens = data.usage.prompt_tokens or 0 -- Default to 0 for disabled model
  local completion_tokens = data.usage.completion_tokens or 0 -- Default to 0 for disabled model

  local formatted_prompt_tokens = string.format("%gk", math.floor(prompt_tokens / 1000))
  local formatted_completion_tokens = string.format("%gk", math.floor(completion_tokens / 1000))

  -- Create the result string with token counts
  local result = data.choices[1].message.content .. '\n\n' .. 'OpenAI ' .. modelUsed .. ' (' .. formatted_prompt_tokens .. ' in, ' .. formatted_completion_tokens .. ' out)\n\n'
  result = common.insertWordToTitle('OPN', result)
  history.saveToHistory('openai_' .. modelUsed , promptToSave .. '\n\n' .. result)

  -- START: Upload the formatted result with public option, using opts
  common.uploadContent(opts.upload_url, opts.upload_token, result, 'OpenAI (' .. modelUsed .. ')', opts.upload_as_public)
  -- END: Upload the formatted result with public option

  return result
end

-- Added a new function to handle and format OpenAI API errors
function query.formatError(status, body)
  common.log("Formatting OpenAI API error: " .. body)
  local error_result
  -- Try to parse the error JSON
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    -- Extract specific error information
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_code = error_data.error.code or ""
    local error_param = error_data.error.param or ""
    -- Build error message with all available details
    error_result = string.format("# OpenAI API Error (%s)\n\n**Error Type**: %s\n", status, error_type)
    if error_code ~= "" then
      error_result = error_result .. string.format("**Error Code**: %s\n", error_code)
    end
    if error_param ~= "" then
      error_result = error_result .. string.format("**Parameter**: %s\n", error_param)
    end
    error_result = error_result .. string.format("**Message**: %s\n", error_message)
  else
    -- Fallback for unexpected error format
    error_result = string.format("# OpenAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError  -- Set our custom error handler
  -- Modified: Pass opts directly to common.askCallback, which then passes it to formatResult
  common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback, upload_url = opts.upload_url, upload_token = opts.upload_token, upload_as_public = opts.upload_as_public}, query.formatResult)
end

local disabled_response = {
  choices = { { message = { content = "OpenAI models are disabled" } } },
  usage = { prompt_tokens = 0, completion_tokens = 0 }
}

-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, query_config)
  promptToSave = prompt
  modelUsed = model

  -- Check if model is disabled
  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {type = 'api key', text = api_key})
  table.insert(body_chunks, {type = 'system instructions', text = instruction})
  table.insert(body_chunks, {role = 'model', text = model})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {type = 'file', filename = context, content = aiconfig.contentOf(context)})
    end
  end
  table.insert(body_chunks, {type = 'prompt', text = prompt})
  -- Removed the final empty table.insert(body_chunks, {}) as its purpose was unclear

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end

-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askLight(model, instruction, prompt, opts, api_key, query_config)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://api.openai.com'
  -- local api_host = 'https://eowloffrpvxwtqp.m.pipedream.net'
  local path = '/v1/chat/completions'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['Authorization'] = 'Bearer ' .. api_key
      },
      body = vim.fn.json_encode(
        {
          model = model,
          messages = (function()
            local messages = {}
            -- This condition is brittle. A better approach would be to configure which models support a dedicated system role.
            if string.sub(model, 1, 2) == 'o1' or string.sub(model, 1, 2) == 'o3' or string.sub(model, 1, 2) == 'o4' then
              table.insert(messages, {role = 'user', content = instruction .. '\n' .. prompt})
            else
              table.insert(messages, { role = 'system', content = instruction })
              table.insert(messages, {role = 'user', content = prompt})
            end
            return messages
          end)(),
          max_tokens = query_config.max_tokens or 8192, -- Use configurable max_tokens
          temperature = query_config.temperature or 0.2, -- Use configurable temperature
          top_p = query_config.top_p or 0.5 -- Use configurable top_p
        }
      ),
      callback = function(res)
        common.log("Before OpenAI callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/anthropic/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

-- Modified: Removed upload_url, upload_token, and upload_as_public parameters from formatResult
function query.formatResult(data, opts)
  common.log("Inside Anthropic formatResult")
  local input_tokens = data.usage.input_tokens or 0
  local output_tokens = data.usage.output_tokens or 0

  local formatted_input_tokens = string.format("%gk", math.floor(input_tokens / 1000))
  local formatted_output_tokens = string.format("%gk", math.floor(output_tokens / 1000))

  -- Create the result string with token counts
  local result = data.content[1].text .. '\n\n' .. 'Anthropic '.. modelUsed .. ' (' .. formatted_input_tokens .. ' in, ' .. formatted_output_tokens .. ' out)\n\n'
  result = common.insertWordToTitle('ANT', result)
  history.saveToHistory('claude_' .. modelUsed , promptToSave .. '\n\n' .. result)

  -- START: Upload the formatted result with public option, using opts
  common.uploadContent(opts.upload_url, opts.upload_token, result, 'Anthropic (' .. modelUsed .. ')', opts.upload_as_public)
  -- END: Upload the formatted result with public option

  return result
end

-- Added a new function to handle and format Anthropic API errors
function query.formatError(status, body)
  common.log("Formatting Anthropic API error: " .. body)
  local error_result
  -- Try to parse the error JSON
  local success, error_data = pcall(vim.fn.json_decode, body)
  if success and error_data and error_data.error then
    -- Extract specific error information
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    error_result = string.format(
      "# Anthropic API Error (%s)\n\n**Error Type**: %s\n**Message**: %s\n",
      status,
      error_type,
      error_message
    )
  else
    -- Fallback for unexpected error format
    error_result = string.format("# Anthropic API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError  -- Set our custom error handler
  -- Modified: Pass opts directly to common.askCallback, which then passes it to formatResult
  common.askCallback(res, {handleResult = opts.handleResult, handleError = handleError, callback = opts.callback, upload_url = opts.upload_url, upload_token = opts.upload_token, upload_as_public = opts.upload_as_public}, query.formatResult)
end

local disabled_response = {
  content = { { text = "Anthropic models are disabled" } },
  usage = { input_tokens = 0, output_tokens = 0 }
}

-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, query_config)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local url = agent_host .. '/anthropic'
  local project_context = aiconfig.listScannedFilesFromConfig()
  local body_chunks = {}
  table.insert(body_chunks, {api_key = api_key})
  table.insert(body_chunks, {system_instruction = instruction})
  table.insert(body_chunks, {role = 'user', content = "I need your help on this project."})
  for _, context in pairs(project_context) do
    if aiconfig.contentOf(context) ~= nil then
      table.insert(body_chunks, {role = 'model', content = "What is the content of `" .. context .. "` ?"})
      table.insert(body_chunks, {role = 'user',  content = "The content of `" .. context .. "` is :\n```\n" .. aiconfig.contentOf(context) .. "\n```"})
    end
  end
  table.insert(body_chunks, {role = 'model', content = "Then what do you want me to do with all that information?"})
  table.insert(body_chunks, {role = 'user', content = prompt})
  table.insert(body_chunks, {model_to_use = model})
  table.insert(body_chunks, {temperature = query_config.temperature or 0.2}) -- Use configurable temperature
  table.insert(body_chunks, {top_p = query_config.top_p or 0.1}) -- Use configurable top_p
  -- Removed the final empty table.insert(body_chunks, {}) as its purpose was unclear

  local function sendNextRequest(i)
    if i > #body_chunks then
      return
    end

    local message = body_chunks[i]
    local body = vim.json.encode(message)

    curl.post(url,
      {
        headers = {['Content-type'] = 'application/json'},
        body = body,
        callback = function(res)
          if i == #body_chunks then
            vim.schedule(function() query.askCallback(res, opts) end)
          else
            sendNextRequest(i + 1)
          end
        end
      })
  end
  sendNextRequest(1)
end


-- Modified: Consolidated upload parameters into opts table, added query_config
function query.askLight(model, instruction, prompt, opts, api_key, query_config)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    vim.schedule(function() query.askCallback({ status = 200, body = vim.json.encode(disabled_response) }, opts) end)
    return
  end

  local api_host = 'https://api.anthropic.com'
  -- local api_host = 'https://eowloffrpvxwtqp.m.pipedream.net'
  local path = '/v1/messages'
  curl.post(api_host .. path,
    {
      headers = {
        ['Content-type'] = 'application/json',
        ['x-api-key'] = api_key,
        ['anthropic-version'] = '2023-06-01'
      },
      body = vim.fn.json_encode(
        {
          model = model,
          max_tokens = query_config.max_tokens or 8192, -- Use configurable max_tokens
          system= instruction,
          messages = (function()
            local messages = {}
            table.insert(messages, {role = 'user', content = prompt})
            return messages
          end)(),
          temperature = query_config.temperature or 0.2, -- Use configurable temperature
          top_p = query_config.top_p or 0.5 -- Use configurable top_p
        }
      ),
      callback = function(res)
        common.log("Before Anthropic callback call")
        vim.schedule(function() query.askCallback(res, opts) end)
      end
    })
end

return query

```

### `lua/ai/common.lua`

```lua
local common = {}
local curl = require('plenary.curl') -- Added curl dependency for upload

local log_file_path = "/tmp/aiconfig.log" -- Default, will be updated by init.lua

function common.set_log_path(path)
  log_file_path = path
end

function common.log(message)
  local timestamp = os.date("%Y-%m-%d %H:%M:%S")
  local full_log_message = "[ " .. timestamp .. " ] -- " .. message .. "\n"

  -- Use native Lua file I/O instead of vim.fn.writefile
  local file, err = io.open(log_file_path, "a")
  if not file then
    -- If logging itself fails, print an error to Neovim's message area.
    -- Using pcall to safely attempt the error write in case we're still in a fast context
    pcall(function()
      vim.api.nvim_echo({{"Error: Could not open log file: " .. log_file_path .. " - " .. (err or "unknown error"), "ErrorMsg"}}, false, {})
    end)
    return
  end
  
  file:write(full_log_message)
  file:close()
end

function common.uploadContent(url, token, content, model_name, is_public)
  if url == '' or token == '' then
    common.log("Upload URL or Token not configured. Skipping upload for " .. model_name .. " response.")
    return -- Early return if not configured
  end

  -- Removed the redundant model_name ~= 'disabled' check here.
  -- The calling query functions should handle disabled models before calling uploadContent.
  common.log("Attempting to upload " .. model_name .. " response to: " .. url)

  local headers = {
    ['Content-Type'] = 'text/markdown',
    ['X-MarkdownBlog-Token'] = token
  }
  -- Add public header if upload_as_public is true
  if is_public == true then
    headers['X-MarkdownBlog-Public'] = 'true'
    common.log("Setting upload as public for " .. model_name)
  end

  common.log("Uploading content for model: " .. model_name)
  curl.put(url,
    {
      headers = headers,
      body = content,
      callback = function(res)
        if res.status >= 200 and res.status < 300 then
          common.log("Successfully uploaded " .. model_name .. " response. Status: " .. res.status)
        else
          common.log("Failed to upload " .. model_name .. " response. Status: " .. res.status .. ", Body: " .. res.body)
        end
      end
    })
end

function common.askCallback(res, opts, formatResult)
  local result
  if res.status ~= 200 then
    if opts.handleError ~= nil then
      result = opts.handleError(res.status, res.body)
    else
      common.log("Error: API responded with the status " .. tostring(res.status) .. '\n\n' .. res.body)
      result = 'Error: API responded with the status ' .. tostring(res.status) .. '\n\n' .. res.body
    end
  else
    local data = vim.fn.json_decode(res.body)
    -- Pass opts directly to formatResult
    result = formatResult(data, opts)
    if opts.handleResult ~= nil then
      result = opts.handleResult(result)
    end
  end
  opts.callback(result)
end

function common.insertWordToTitle(word_to_insert, text)
  local lines = vim.split(text, '\n', { plain = true })
  if #lines == 0 then
    return text
  end

  -- Check if the first line starts with a title
  if lines[1]:sub(1, 1) == '#' then
    -- Insert the word at the beginning of the first line
    lines[1] = lines[1]:gsub('^# ', '# ' .. word_to_insert .. ' ')
  else
    -- Prepend a new title with the word
    lines[1] = '# ' .. word_to_insert .. ' ' .. lines[1]
  end

  return table.concat(lines, '\n')
end

return common

```

GoogleAI gemini-2.5-flash (17k in, 13k out)


