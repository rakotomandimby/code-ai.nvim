# Identify code that can be factorized in a common function

I want you to review the code and tell me if there is some code in the 3 `query.lua` files that can be factorized in a common function.
If there is some code that can be factorized, provide me the refactored code with the common function.
Tell me the full new contents of the files I need to create and modify.


# ANT After reviewing the three `query.lua` files, I've identified several areas that can be factorized into common functions. Here's my analysis and refactored code:

## Code That Can Be Factorized

1. **Token formatting logic** - All three files format token counts similarly (e.g., "30k", "2k")
2. **Disabled model response handling** - All three files have nearly identical logic for handling disabled models
3. **Heavy query logic** - The `askHeavy` functions are almost identical across all three providers
4. **Upload content calls** - All three files call `common.uploadContent` with similar patterns

## Refactored Solution

### 1. Create/Modify `lua/ai/common.lua`

Add these new functions to the existing `common.lua` file:

```lua
local common = {}
local curl = require('plenary.curl')

function common.log(message)
  local log_path = "/tmp/aiconfig.log"
  local timestamp = os.date("%Y-%m-%d %H:%M:%S")
  local full_log_message = "[ " .. timestamp .. " ] -- " .. message .. "\n"

  local file, err = io.open(log_path, "a")
  if not file then
    pcall(function()
      vim.api.nvim_echo({{"Error: Could not open log file: " .. log_path .. " - " .. (err or "unknown error"), "ErrorMsg"}}, false, {})
    end)
    return
  end
  
  file:write(full_log_message)
  file:close()
end

function common.uploadContent(url, token, content, model_name, is_public)
  if url == '' or token == '' then
    common.log("Upload URL or Token not configured. Skipping upload for " .. model_name .. " response.")
    return
  end

  if model_name ~= 'disabled' then
    common.log("Attempting to upload " .. model_name .. " response to: " .. url)

    local headers = {
      ['Content-Type'] = 'text/markdown',
      ['X-MarkdownBlog-Token'] = token
    }
    if is_public == true then
      headers['X-MarkdownBlog-Public'] = 'true'
      common.log("Setting upload as public for " .. model_name)
    end

    common.log("Uploading content for model: " .. model_name)
    curl.put(url,
      {
        headers = headers,
        body = content,
        callback = function(res)
          if res.status >= 200 and res.status < 300 then
            common.log("Successfully uploaded " .. model_name .. " response. Status: " .. res.status)
          else
            common.log("Failed to upload " .. model_name .. " response. Status: " .. res.status .. ", Body: " .. res.body)
          end
        end
      })
  else
    common.log("Model is disabled. Skipping upload.")
  end
end

function common.askCallback(res, opts, formatResult)
  local result
  if res.status ~= 200 then
    if opts.handleError ~= nil then
      result = opts.handleError(res.status, res.body)
    else
      common.log("Error: API responded with the status " .. tostring(res.status) .. '\n\n' .. res.body)
      result = 'Error: API responded with the status ' .. tostring(res.status) .. '\n\n' .. res.body
    end
  else
    local data = vim.fn.json_decode(res.body)
    result = formatResult(data, opts.upload_url, opts.upload_token, opts.upload_as_public)
    if opts.handleResult ~= nil then
      result = opts.handleResult(result)
    end
  end
  opts.callback(result)
end

function common.insertWordToTitle(word_to_insert, text)
  local lines = vim.split(text, '\n', { plain = true })
  if #lines == 0 then
    return text
  end

  if lines[1]:sub(1, 1) == '#' then
    lines[1] = lines[1]:gsub('^# ', '# ' .. word_to_insert .. ' ')
  else
    lines[1] = '# ' .. word_to_insert .. ' ' .. lines[1]
  end

  return table.concat(lines, '\n')
end

-- NEW: Format token counts for display
function common.formatTokenCount(count)
  if type(count) ~= 'number' then
    count = tonumber(count) or 0
  end
  
  if count >= 1000 then
    local value = count / 1000
    if value >= 100 then
      return string.format("%.0fk", value)
    elseif value >= 10 then
      return string.format("%.1fk", value)
    else
      return string.format("%.2fk", value)
    end
  end
  return tostring(count)
end

-- NEW: Handle disabled model response
function common.handleDisabledModel(provider_name, model_name, opts, askCallback, disabled_response)
  vim.schedule(function()
    askCallback(
      { status = 200, body = vim.json.encode(disabled_response) },
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = opts.upload_url or '',
        upload_token = opts.upload_token or '',
        upload_as_public = opts.upload_as_public or false
      }
    )
  end)
end

-- NEW: Generic heavy query implementation
function common.askHeavy(agent_host, api_key, model, instruction, prompt, project_context, opts, askCallback)
  local url = agent_host .. '/'
  local body_chunks = {}
  
  table.insert(body_chunks, {type = 'api key', text = api_key})
  table.insert(body_chunks, {type = 'system instructions', text = instruction})
  table.insert(body_chunks, {type = 'model', text = model})
  
  for _, context in pairs(project_context) do
    if context.content ~= nil then
      table.insert(body_chunks, {type = 'file', filename = context.filename, content = context.content})
    end
  end
  
  table.insert(body_chunks, {type = 'prompt', text = prompt})

  -- Send all chunks except the last without waiting
  for i = 1, #body_chunks - 1 do
    local message = body_chunks[i]
    local body = vim.json.encode(message)
    curl.post(url, {
      headers = {['Content-type'] = 'application/json'},
      body = body,
      callback = function(res) end
    })
  end

  -- Send the last chunk and wait for response
  local last_message = body_chunks[#body_chunks]
  local body = vim.json.encode(last_message)

  curl.post(url, {
    headers = {['Content-type'] = 'application/json'},
    body = body,
    callback = function(res)
      vim.schedule(function()
        askCallback(res, {
          handleResult = opts.handleResult,
          callback = opts.callback,
          upload_url = opts.upload_url,
          upload_token = opts.upload_token,
          upload_as_public = opts.upload_as_public
        })
      end)
    end
  })
end

return common
```

### 2. Refactored `lua/ai/anthropic/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data, upload_url, upload_token, upload_as_public)
  common.log("Inside Anthropic formatResult")
  
  local input_tokens = data.usage.input_tokens or 0
  local output_tokens = data.usage.output_tokens or 0

  local formatted_input_tokens = common.formatTokenCount(input_tokens)
  local formatted_output_tokens = common.formatTokenCount(output_tokens)

  local result = data.content[1].text 
    .. '\n\n' 
    .. 'Anthropic ' .. modelUsed 
    .. ' (' .. formatted_input_tokens .. ' in, ' .. formatted_output_tokens .. ' out)\n\n'
  
  result = common.insertWordToTitle('ANT', result)
  history.saveToHistory('claude_' .. modelUsed, promptToSave .. '\n\n' .. result)

  common.uploadContent(upload_url, upload_token, result, 'Anthropic (' .. modelUsed .. ')', upload_as_public)

  return result
end

function query.formatError(status, body)
  common.log("Formatting Anthropic API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    error_result = string.format(
      "# Anthropic API Error (%s)\n\n**Error Type**: %s\n**Message**: %s\n",
      status,
      error_type,
      error_message
    )
  else
    error_result = string.format("# Anthropic API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError
  common.askCallback(
    res, 
    {
      handleResult = opts.handleResult, 
      handleError = handleError, 
      callback = opts.callback, 
      upload_url = opts.upload_url, 
      upload_token = opts.upload_token, 
      upload_as_public = opts.upload_as_public
    }, 
    query.formatResult
  )
end

local disabled_response = {
  content = { { text = "Anthropic models are disabled" } },
  usage = { input_tokens = 0, output_tokens = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('Anthropic', model, 
      {
        handleResult = opts.handleResult, 
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      }, 
      query.askCallback, 
      disabled_response
    )
    return
  end

  local scanned_files = aiconfig.listScannedFilesFromConfig()
  local project_context = {}
  
  for _, context in pairs(scanned_files) do
    local content = aiconfig.contentOf(context)
    if content ~= nil then
      table.insert(project_context, {filename = context, content = content})
    end
  end

  common.askHeavy(
    agent_host,
    api_key,
    model,
    instruction,
    prompt,
    project_context,
    {
      handleResult = opts.handleResult,
      callback = opts.callback,
      upload_url = upload_url,
      upload_token = upload_token,
      upload_as_public = upload_as_public
    },
    query.askCallback
  )
end

function query.askLight(model, instruction, prompt, opts, api_key, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('Anthropic', model,
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      },
      query.askCallback,
      disabled_response
    )
    return
  end

  local api_host = 'https://api.anthropic.com'
  local path = '/v1/messages'
  
  curl.post(api_host .. path, {
    headers = {
      ['Content-type'] = 'application/json',
      ['x-api-key'] = api_key,
      ['anthropic-version'] = '2023-06-01'
    },
    body = vim.fn.json_encode({
      model = model,
      max_tokens = 8192,
      system = instruction,
      messages = {{role = 'user', content = prompt}}
    }),
    callback = function(res)
      common.log("Before Anthropic callback call")
      vim.schedule(function()
        query.askCallback(res, {
          handleResult = opts.handleResult,
          callback = opts.callback,
          upload_url = upload_url,
          upload_token = upload_token,
          upload_as_public = upload_as_public
        })
      end)
    end
  })
end

return query
```

### 3. Refactored `lua/ai/googleai/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data, upload_url, upload_token, upload_as_public)
  common.log("Inside GoogleAI formatResult")
  
  local result = ''
  local candidates_number = #data['candidates']
  
  if candidates_number == 1 then
    if data['candidates'][1]['content'] == nil then
      result = '\n#GoogleAI error\n\nGoogleAI stopped with the reason: ' 
        .. data['candidates'][1]['finishReason'] .. '\n'
      return result
    else
      local prompt_tokens = data['usageMetadata']['promptTokenCount'] or 0
      local answer_tokens = data['usageMetadata']['candidatesTokenCount'] or 0

      local formatted_prompt_tokens = common.formatTokenCount(prompt_tokens)
      local formatted_answer_tokens = common.formatTokenCount(answer_tokens)

      result = data['candidates'][1]['content']['parts'][1]['text'] 
        .. '\n\n' 
        .. 'GoogleAI ' .. modelUsed 
        .. ' (' .. formatted_prompt_tokens .. ' in, ' .. formatted_answer_tokens .. ' out)\n\n'
    end
  else
    result = '# There are ' .. candidates_number .. ' GoogleAI candidates\n'
    for i = 1, candidates_number do
      result = result .. '## GoogleAI Candidate number ' .. i .. '\n'
      result = result .. data['candidates'][i]['content']['parts'][1]['text'] .. '\n'
    end
  end
  
  result = common.insertWordToTitle('GGL', result)
  history.saveToHistory('googleai_' .. modelUsed, promptToSave .. '\n\n' .. result)

  common.uploadContent(upload_url, upload_token, result, 'GoogleAI (' .. modelUsed .. ')', upload_as_public)

  return result
end

function query.formatError(status, body)
  common.log("Formatting GoogleAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  
  if success and error_data and error_data.error then
    local error_code = error_data.error.code or status
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_status = error_data.error.status or "ERROR"
    error_result = string.format(
      "# GoogleAI API Error (%s)\n\n**Error Code**: %s\n**Status**: %s\n**Message**: %s\n",
      status,
      error_code,
      error_status,
      error_message
    )
  else
    error_result = string.format("# GoogleAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError
  common.askCallback(
    res,
    {
      handleResult = opts.handleResult,
      handleError = handleError,
      callback = opts.callback,
      upload_url = opts.upload_url,
      upload_token = opts.upload_token,
      upload_as_public = opts.upload_as_public
    },
    query.formatResult
  )
end

local disabled_response = {
  candidates = { { content = { parts = { { text = "GoogleAI models are disabled" } } }, finishReason = "STOP" } },
  usageMetadata = { promptTokenCount = 0, candidatesTokenCount = 0 }
}

function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('GoogleAI', model,
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      },
      query.askCallback,
      disabled_response
    )
    return
  end

  local scanned_files = aiconfig.listScannedFilesFromConfig()
  local project_context = {}
  
  for _, context in pairs(scanned_files) do
    local content = aiconfig.contentOf(context)
    if content ~= nil then
      table.insert(project_context, {filename = context, content = content})
    end
  end

  common.askHeavy(
    agent_host,
    api_key,
    model,
    instruction,
    prompt,
    project_context,
    {
      handleResult = opts.handleResult,
      callback = opts.callback,
      upload_url = upload_url,
      upload_token = upload_token,
      upload_as_public = upload_as_public
    },
    query.askCallback
  )
end

function query.askLight(model, instruction, prompt, opts, api_key, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('GoogleAI', model,
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      },
      query.askCallback,
      disabled_response
    )
    return
  end

  local api_host = 'https://generativelanguage.googleapis.com'
  local path = '/v1beta/models/' .. model .. ':generateContent'
  
  curl.post(api_host .. path, {
    headers = {
      ['Content-type'] = 'application/json',
      ['x-goog-api-key'] = api_key
    },
    body = vim.fn.json_encode({
      system_instruction = {parts = {text = instruction}},
      contents = {{role = 'user', parts = {{text = prompt}}}},
      safetySettings = {
        { category = 'HARM_CATEGORY_SEXUALLY_EXPLICIT', threshold = 'BLOCK_NONE' },
        { category = 'HARM_CATEGORY_HATE_SPEECH',       threshold = 'BLOCK_NONE' },
        { category = 'HARM_CATEGORY_HARASSMENT',        threshold = 'BLOCK_NONE' },
        { category = 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold = 'BLOCK_NONE' }
      },
      generationConfig = {
        temperature = 0.2,
        topP = 0.5
      }
    }),
    callback = function(res)
      vim.schedule(function()
        query.askCallback(res, {
          handleResult = opts.handleResult,
          callback = opts.callback,
          upload_url = upload_url,
          upload_token = upload_token,
          upload_as_public = upload_as_public
        })
      end)
    end
  })
end

return query
```

### 4. Refactored `lua/ai/openai/query.lua`

```lua
local curl = require('plenary.curl')
local aiconfig = require('ai.aiconfig')
local common = require('ai.common')
local query = {}
local history = require('ai.history')

local promptToSave = ""
local modelUsed = ""

function query.formatResult(data, upload_url, upload_token, upload_as_public)
  common.log("Inside OpenAI formatResult")

  local function collect_texts(d)
    local out = {}

    if type(d.output_text) == 'string' and d.output_text ~= '' then
      table.insert(out, d.output_text)
    elseif type(d.output_text) == 'table' then
      for _, s in ipairs(d.output_text) do
        if type(s) == 'string' and s ~= '' then table.insert(out, s) end
      end
    end

    if type(d.output) == 'table' then
      for _, item in ipairs(d.output) do
        if type(item) == 'table' then
          if type(item.text) == 'string' and item.text ~= '' then
            table.insert(out, item.text)
          end
          if type(item.content) == 'table' then
            for _, part in ipairs(item.content) do
              if type(part) == 'table' then
                local t = part.text or part.value
                if type(t) == 'string' and t ~= '' then
                  table.insert(out, t)
                end
              elseif type(part) == 'string' and part ~= '' then
                table.insert(out, part)
              end
            end
          elseif type(item.content) == 'string' and item.content ~= '' then
            table.insert(out, item.content)
          end
        elseif type(item) == 'string' and item ~= '' then
          table.insert(out, item)
        end
      end
    end

    return out
  end

  local prompt_tokens = type(data.usage) == 'table' and (data.usage.input_tokens or 0) or 0
  local completion_tokens = type(data.usage) == 'table' and (data.usage.output_tokens or 0) or 0
  
  local formatted_prompt_tokens = common.formatTokenCount(prompt_tokens)
  local formatted_completion_tokens = common.formatTokenCount(completion_tokens)

  local pieces = collect_texts(data)
  local text = table.concat(pieces, "\n\n")

  local result = text
    .. '\n\n'
    .. 'OpenAI ' .. modelUsed 
    .. ' (' .. formatted_prompt_tokens .. ' in, ' .. formatted_completion_tokens .. ' out)\n\n'

  result = common.insertWordToTitle('OPN', result)
  history.saveToHistory('openai_' .. modelUsed, promptToSave .. '\n\n' .. result)

  local model_label = (modelUsed == 'disabled') and 'disabled' or ('OpenAI (' .. modelUsed .. ')')
  common.uploadContent(upload_url, upload_token, result, model_label, upload_as_public)

  return result
end

function query.formatError(status, body)
  common.log("Formatting OpenAI API error: " .. body)
  local error_result
  local success, error_data = pcall(vim.fn.json_decode, body)
  
  if success and error_data and error_data.error then
    local error_type = error_data.error.type or "unknown_error"
    local error_message = error_data.error.message or "Unknown error occurred"
    local error_code = error_data.error.code or ""
    local error_param = error_data.error.param or ""
    
    error_result = string.format("# OpenAI API Error (%s)\n\n**Error Type**: %s\n", status, error_type)
    if error_code ~= "" then
      error_result = error_result .. string.format("**Error Code**: %s\n", error_code)
    end
    if error_param ~= "" then
      error_result = error_result .. string.format("**Parameter**: %s\n", error_param)
    end
    error_result = error_result .. string.format("**Message**: %s\n", error_message)
  else
    error_result = string.format("# OpenAI API Error (%s)\n\n```\n%s\n```", status, body)
  end
  return error_result
end

query.askCallback = function(res, opts)
  local handleError = query.formatError
  common.askCallback(
    res,
    {
      handleResult = opts.handleResult,
      handleError = handleError,
      callback = opts.callback,
      upload_url = opts.upload_url,
      upload_token = opts.upload_token,
      upload_as_public = opts.upload_as_public
    },
    query.formatResult
  )
end

local disabled_response = {
  output = {
    { type = "message", role = "assistant", content = { { type = "output_text", text = "" } } },
    { type = "message", role = "assistant", content = { { type = "output_text", text = "OpenAI models are disabled" } } },
  },
  usage = {
    input_tokens = 0,
    output_tokens = 0,
    total_tokens = 0,
  },
}

function query.askHeavy(model, instruction, prompt, opts, api_key, agent_host, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('OpenAI', model,
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      },
      query.askCallback,
      disabled_response
    )
    return
  end

  local scanned_files = aiconfig.listScannedFilesFromConfig()
  local project_context = {}
  
  for _, context in pairs(scanned_files) do
    local content = aiconfig.contentOf(context)
    if content ~= nil then
      table.insert(project_context, {filename = context, content = content})
    end
  end

  common.askHeavy(
    agent_host,
    api_key,
    model,
    instruction,
    prompt,
    project_context,
    {
      handleResult = opts.handleResult,
      callback = opts.callback,
      upload_url = upload_url,
      upload_token = upload_token,
      upload_as_public = upload_as_public
    },
    query.askCallback
  )
end

function query.askLight(model, instruction, prompt, opts, api_key, upload_url, upload_token, upload_as_public)
  promptToSave = prompt
  modelUsed = model

  if model == "disabled" then
    common.handleDisabledModel('OpenAI', model,
      {
        handleResult = opts.handleResult,
        callback = opts.callback,
        upload_url = upload_url,
        upload_token = upload_token,
        upload_as_public = upload_as_public
      },
      query.askCallback,
      disabled_response
    )
    return
  end

  local api_host = 'https://api.openai.com'
  local path = '/v1/responses'

  local input_messages = {
    {
      role = 'user',
      content = {
        { type = 'input_text', text = prompt }
      }
    }
  }

  curl.post(api_host .. path, {
    headers = {
      ['Content-type'] = 'application/json',
      ['Authorization'] = 'Bearer ' .. api_key,
    },
    body = vim.fn.json_encode({
      model = model,
      instructions = instruction,
      input = input_messages,
    }),
    callback = function(res)
      common.log("Before OpenAI callback call (Responses API)")
      vim.schedule(function()
        query.askCallback(res, {
          handleResult = opts.handleResult,
          callback = opts.callback,
          upload_url = upload_url,
          upload_token = upload_token,
          upload_as_public = upload_as_public
        })
      end)
    end
  })
end

return query
```

## Summary of Changes

### New Common Functions Added:
1. **`common.formatTokenCount(count)`** - Formats token counts consistently across all providers
2. **`common.handleDisabledModel(...)`** - Handles disabled model responses uniformly
3. **`common.askHeavy(...)`** - Generic implementation for heavy queries with agent support

### Benefits:
- **Reduced code duplication**: ~150 lines of duplicate code eliminated
- **Easier maintenance**: Changes to heavy query logic only need to be made in one place
- **Consistency**: All providers now format tokens and handle disabled models identically
- **Cleaner code**: Each provider's query file is more focused on provider-specific logic

The refactored code maintains all existing functionality while being more maintainable and DRY (Don't Repeat Yourself).

Anthropic claude-sonnet-4-5-20250929 (14k in, 7k out)


